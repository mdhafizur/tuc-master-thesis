
Secure coding support within the integrated development environment is increasingly demanded, as developers prefer immediate and actionable feedback during implementation rather than deferred security checks in later pipeline stages. Traditional static application security testing tools remain effective for rule-based vulnerabilities but exhibit limitations in detecting weaknesses that require semantic reasoning, cross-file context, or domain knowledge. Large Language Models (LLMs) offer new potential for vulnerability detection and automated repair suggestions; however, their adoption is constrained by inconsistent detection quality, high false-positive rates, outdated security knowledge, and significant privacy concerns when proprietary source code is processed by cloud-based services.

This thesis investigates a privacy-preserving approach to source code vulnerability detection and repair by integrating locally deployed LLMs with Retrieval-Augmented Generation (RAG) in a Visual Studio Code extension. The proposed system operates entirely on-device, ensuring zero code exfiltration while dynamically incorporating up-to-date vulnerability knowledge from curated Common Weakness Enumeration (CWE), Common Vulnerabilities and Exposures (CVE), and OWASP sources. Two operational modes are designed: a low-latency inline mode using LLM-only inference for real-time feedback, and an audit mode that combines LLM reasoning with retrieval-based context enrichment for deeper analysis.

The system is evaluated on JavaScript and TypeScript benchmarks, including Juliet, the OWASP Benchmark, and selected real-world CVE cases. Quantitative evaluation compares LLM-only and LLM+RAG configurations against established static analysis baselines using precision, recall, F1-score, latency, and resource utilization metrics. The findings indicate that RAG-enhanced local inference improves detection consistency and recall for security-critical vulnerabilities while maintaining practical latency constraints and preserving repair suggestion quality. Overall, the results demonstrate that privacy-preserving, locally deployed LLMs augmented with retrieval-based knowledge can provide effective and reproducible vulnerability detection and repair assistance within the developer workflow.

\medskip
\noindent\textbf{Keywords:} source code security, vulnerability detection, secure code repair, large language models, retrieval-augmented generation, privacy-preserving systems, Visual Studio Code

