This chapter summarizes the contributions of this thesis, highlights key findings, discusses limitations, and outlines implications for practice. The goal of the thesis was to design and implement a privacy-preserving vulnerability detection and repair assistant for Visual Studio Code that leverages locally deployed LLMs and retrieval-augmented grounding without transmitting source code to external services.

\section{Summary of Contributions}

\textbf{Code Guardian: an IDE-integrated, local security assistant.}
This thesis delivers \textbf{Code Guardian}, a VS Code extension that performs on-device vulnerability analysis for JavaScript and TypeScript projects. Findings are presented using IDE-native diagnostics, and optional quick fixes provide repair suggestions while keeping developers in full control of code changes.

\textbf{Privacy-preserving LLM inference with optional RAG.}
All code analysis runs locally via Ollama. To improve grounding, the system optionally augments prompts with locally retrieved security knowledge (CWE/OWASP/CVE guidance) using a local vector index and local embeddings. This architecture supports explainability and consistency while preserving the no-exfiltration requirement.

\textbf{Practical performance mechanisms.}
To remain usable during development, Code Guardian combines debounced triggers, function-level scoping for real-time use, and caching of repeated analyses. These mechanisms reduce unnecessary inference calls and support responsive IDE feedback.

\textbf{Reproducible evaluation harness.}
The prototype includes a curated benchmark of security test cases and a local evaluation script for comparing models and configurations using standard detection metrics, parse robustness, and latency.

\section{Answers to Research Questions}

\textbf{RQ1 (Feasibility).}
The thesis supports feasibility with caveats. Code Guardian demonstrates that useful vulnerability analysis and repair suggestions can be generated inside VS Code using fully local inference, satisfying the no-code-exfiltration objective. However, practical usefulness depends on selecting a model profile that matches the deployment context (interactive editing vs.\ audit workflows).

\textbf{RQ2 (Grounding).}
Retrieval augmentation is beneficial only under model-dependent conditions. In this run, RAG improved \texttt{qwen3:8b} (F1 35.64\% $\rightarrow$ 42.70\%) and slightly improved \texttt{gemma3:4b} (F1 30.67\% $\rightarrow$ 31.37\%), but reduced \texttt{gemma3:1b} recall to zero true positives. Grounding should therefore be treated as a tunable strategy rather than a universally positive default.

\textbf{RQ3 (Practicality).}
IDE practicality is achievable when inference is constrained by function-level scoping, debounced triggers, and caching. These mechanisms keep local analysis responsive for small models, while larger models remain better suited to on-demand scans where higher latency is acceptable.

\section{Deployment Implications}

Table~\ref{tab:conclusion-deployment-profiles} summarizes pragmatic deployment choices based on the measured trade-offs.

\begin{table}[H]
  \centering
  \caption{Recommended deployment profiles from thesis results.}
  \label{tab:conclusion-deployment-profiles}
  \small
  \begin{tabularx}{\textwidth}{p{0.19\textwidth}p{0.22\textwidth}XX}
    \toprule
    Use case & Preferred configuration & Main advantage & Main risk \\
    \midrule
    Real-time editor feedback & \texttt{gemma3:1b} (LLM-only) & Very low latency (about 178 ms median) and stable parsing & Misses many vulnerabilities (recall 5.56\%) \\
    Moderate-latency audit pass & \texttt{gemma3:4b} (LLM+RAG) & Higher recall (44.44\%) with about 1.3 s median latency & Severe alert noise on secure samples (FPR 100\%) \\
    Higher-precision deep audit & \texttt{qwen3:8b} (LLM+RAG) & Best F1 (42.70\%) and lower FPR (27.12\%) than other high-recall modes & Multi-second latency (median 7809 ms) and non-perfect parse rate (84.85\%) \\
    Unstable model profiles & \texttt{qwen3:4b} / \texttt{CodeLlama:latest} & N/A in this run & Parse-collapse regimes (1.01--5.05\% parse), yielding near-zero recall \\
    \bottomrule
  \end{tabularx}
\end{table}

\section{Limitations}

\textbf{Scope limits and contextual depth.}
While the system can flag common vulnerability patterns, deep semantic reasoning across files (e.g., source-to-sink flows spanning modules) is limited by the analysis scope and the absence of full static data-flow analysis.

\textbf{Evaluation representativeness.}
The curated dataset is intentionally small and human-auditable, but it does not fully reflect the diversity and ambiguity of real-world codebases. Results should therefore be interpreted as indicative rather than definitive.

\textbf{Repair correctness.}
Repair suggestions are model-generated and may affect behavior beyond security hardening. The system mitigates this by requiring explicit user review, but comprehensive functional validation remains outside the scope of the extension.

\section{Responsible Use}

This thesis treats Code Guardian as a decision-support system, not an autonomous security verifier. Both false negatives and false positives were observed, and some findings used semantically plausible but ontology-mismatched labels. For this reason, findings and repair suggestions should remain reviewable artifacts under developer control, with conventional testing and security review retained as mandatory safeguards before release.

\section{Conclusion}

Privacy-preserving secure coding assistance is feasible within the IDE when local LLM inference is combined with careful prompt structuring, optional retrieval grounding, and developer-controlled remediation workflows. The evaluation shows a clear quality-latency-robustness trade-off: \texttt{gemma3:1b} remained fast (about 178--180 ms median) but had very low recall, \texttt{gemma3:4b} increased recall at around 1.3--1.4 s median latency but over-warned on secure code, and \texttt{qwen3:8b} achieved the best F1 with markedly higher multi-second latency.

The results also show that retrieval augmentation is not universally beneficial; its effect is model-dependent. In this study, RAG improved \texttt{qwen3:8b} and marginally improved \texttt{gemma3:4b}, but reduced \texttt{gemma3:1b} to zero true positives and did not recover parse-collapse regimes for \texttt{qwen3:4b} and \texttt{CodeLlama:latest}. This indicates that RAG integration must be calibrated per model and prompt format, not assumed to improve security detection by default.

Overall, Code Guardian demonstrates that locally deployed LLMs can provide useful vulnerability detection and repair suggestions without transmitting source code off-device, but practical deployment requires explicit configuration choices for model class, latency budget, and acceptable false-positive behavior.
