Building on the contributions and limitations above, several next steps stand out for improving Code Guardianâ€™s effectiveness, robustness, and evaluation depth.

\noindent\textbf{Deeper program analysis and cross-file context.}
Add lightweight static analysis to extract taint-style source-to-sink traces across functions and files, and feed these traces into the LLM prompt. This can reduce false positives from missing context and improve recall for vulnerabilities that span modules (e.g., validation in one file and sink usage in another).

\medskip

\noindent\textbf{Hybrid integration with traditional SAST.}
Integrate rule-based baselines (e.g., Semgrep) as an additional signal. A hybrid approach can use SAST findings as candidate locations and let the LLM focus on contextual reasoning and repair generation, improving both precision and developer trust.

\medskip

\noindent\textbf{Repair validation and safer patching.}
Extend repair suggestions with syntactic checks and minimal local validation (e.g., TypeScript typechecking on modified regions). Provide diff previews by default and track when suggested fixes introduce new warnings.

\medskip

\noindent\textbf{Adversarial robustness.}
Study prompt-injection and retrieval-poisoning risks within the IDE context (e.g., attacker-controlled comments or dependency code). Add provenance and filtering for retrieved knowledge, and implement safe prompt templates that explicitly treat code as data.

\medskip

\noindent\textbf{Broader evaluation on standard benchmarks and real repositories.}
Complement the curated dataset with larger benchmarks (e.g., OWASP Benchmark, Juliet-style suites) and selected real-world CVE cases. Report confidence intervals, paired significance tests (e.g., McNemar/permutation on per-sample outcomes), and per-category breakdowns, and compare against SAST baselines under matched conditions.

\medskip

\noindent\textbf{User studies in realistic IDE workflows.}
Run a developer study to measure time-to-fix, perceived usefulness, trust calibration, and false-positive tolerance. Standard usability and workload instruments such as SUS and NASA-TLX can complement objective metrics \cite{brooke1996sus,hart1988nasa}. Compare LLM-only vs.\ LLM+RAG configurations under real editing sessions to validate R6 and practical adoption constraints.
