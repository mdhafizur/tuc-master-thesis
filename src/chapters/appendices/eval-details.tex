\section{Curated Test Suite Overview}
\label{app:dataset-overview}

The curated evaluation suite maintained in \texttt{code-guardian/evaluation/datasets/} contains representative vulnerability snippets for JavaScript/TypeScript secure coding. Expected vulnerabilities include CWE identifiers and severities so results can be aggregated by category.

\subsection*{Dataset sizes}

For the scored thesis run in Chapter~\ref{chap:evaluation}, the harness uses:
\begin{itemize}
  \item \textbf{Primary scored dataset:} \texttt{vulnerability-test-cases.json} with 33 test cases (18 vulnerable, 15 secure)
\end{itemize}
Additional datasets can be added for broader coverage, but all quantitative results in this thesis are derived from the 33-case scored dataset above.

\subsection*{Representative Vulnerability Classes}

The dataset includes (non-exhaustive) examples for:
\begin{itemize}
  \item SQL injection (CWE-89)
  \item Cross-site scripting (CWE-79)
  \item Command injection (CWE-78)
  \item Path traversal (CWE-22)
  \item Insecure randomness (CWE-338)
  \item Hardcoded credentials (CWE-798)
  \item CSRF and authentication-flow weaknesses (CWE-352, CWE-287)
  \item Prototype pollution / unsafe reflection patterns (CWE-1321, CWE-470)
\end{itemize}

\subsection*{Test Case Record Format}

Each test case contains:
\begin{itemize}
  \item a code snippet (\texttt{code}),
  \item a list of expected findings (\texttt{expectedVulnerabilities}),
  \item and optional remediation guidance (\texttt{expectedFix}).
\end{itemize}

\subsection*{Reproducing the harness run}

The evaluation script is executed locally:
\begin{lstlisting}[language=Java, caption={Running the evaluation script}, label={lst:appendix-eval-run}]
cd code-guardian
node evaluation/evaluate-models.js
\end{lstlisting}

The script prints per-model precision/recall/F1, false positive rate, average response time, and JSON parse success rate. Models to test are specified in the script and can be edited to match the locally installed Ollama models.

\subsection*{Recommended Artifact Checklist}

For full reproducibility and auditability, the following files should be archived with the thesis:
\begin{itemize}
  \item Raw run output JSON from the harness (all configurations and per-case results)
  \item Exact run configuration object (models, prompt modes, runtime parameters)
  \item Model digests / quantization metadata used for inference
  \item Environment metadata (OS, Node.js, Ollama versions, hardware)
  \item Generated tables or scripts used to derive reported confidence intervals and significance tests
\end{itemize}

This appendix is intentionally concise: the full dataset is machine-readable and can be inspected directly in the repository.
