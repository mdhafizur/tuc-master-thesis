Modern software systems form the foundation of critical infrastructure, enterprise platforms, and consumer-facing applications. As software continues to increase in size, complexity, and development velocity, vulnerabilities introduced during implementation remain one of the dominant attack vectors. Empirical studies indicate that a substantial fraction of reported security flaws originate from defects introduced during the software development phase, including injection vulnerabilities, improper input validation, insecure deserialization, and flawed access control logic \cite{pearce2022copilot,fu2023llmsec}. These weaknesses persist despite widespread adoption of secure development practices and automated analysis tools.

Static Application Security Testing (SAST) tools are commonly employed to detect vulnerabilities early in the Software Development Life Cycle (SDLC). Rule-based and taint-analysis-driven analyzers provide deterministic results and are effective for well-specified vulnerability patterns. However, prior work highlights several limitations, including high false-positive rates, limited contextual and semantic reasoning, and difficulty generalizing across diverse coding styles and frameworks \cite{sheng2025survey,li2025iris}. As a result, developers frequently face large volumes of security findings that are costly to triage and may not reflect exploitable vulnerabilities.

In contemporary development workflows, security findings are often surfaced under significant time pressure. When a vulnerability is flagged, developers must interpret diagnostic output, locate the root cause, and apply an appropriate fix while preserving functional correctness. This process is cognitively demanding and error-prone, particularly for developers without specialized security expertise. Studies have shown that security warnings are frequently ignored, misinterpreted, or resolved incorrectly, reducing the practical effectiveness of traditional static analysis tools \cite{fu2023llmsec,li2025everything}.

Recent advances in Large Language Models (LLMs) have demonstrated strong capabilities in source code understanding, generation, and transformation. LLMs can reason over programming language syntax and semantics, enabling them to identify potential defects and generate candidate repair suggestions. Consequently, LLMs have attracted significant attention as potential tools for vulnerability detection and automated repair \cite{he2023safecoder,islam2024secrepair}. However, multiple empirical evaluations indicate that LLM-generated or LLM-modified code is frequently \emph{functionally correct yet insecure}, introducing subtle vulnerabilities that are difficult to detect without explicit security grounding \cite{pearce2022copilot,peng2025cweval}. This discrepancy between functional correctness and security correctness represents a fundamental challenge for applying LLMs in secure software development.

Traditional SAST tools and LLM-based approaches exhibit complementary strengths and weaknesses. Static analyzers offer deterministic behavior and clear vulnerability specifications but lack deep contextual reasoning and adaptability. In contrast, LLMs provide strong contextual understanding and flexible reasoning but lack explicit security constraints and may hallucinate insecure solutions \cite{sheng2025survey,kavian2024llmsecguard}. This tension suggests that neither approach is sufficient in isolation for reliable vulnerability detection and repair.

Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these limitations by grounding LLM reasoning in external security knowledge. By augmenting model prompts with retrieved vulnerability descriptions, secure coding guidelines, and historical fixes, RAG-based systems can improve detection accuracy and reasoning consistency without requiring model retraining \cite{mialon2023augmented,shi2025rescue}. Nevertheless, most existing RAG-based secure coding systems rely on cloud-hosted models or external services, raising significant privacy and confidentiality concerns. Source code often contains proprietary logic or sensitive information, making cloud-based analysis unsuitable for regulated or security-critical environments \cite{kaur2025cyberreview,gholami2024llmcyber}.

These concerns motivate the development of privacy-preserving, locally deployed LLM systems integrated directly into the Integrated Development Environment (IDE). Local deployment enables developers to benefit from advanced reasoning capabilities while preserving code confidentiality, reducing data leakage risks, and improving reproducibility. Despite increasing interest, the design and systematic evaluation of such systems—particularly those combining retrieval, static analysis, and LLM-based reasoning within the IDE—remain underexplored \cite{basic2025slr}.

This thesis addresses this gap by investigating a privacy-preserving vulnerability detection and repair system based on retrieval-augmented local LLMs integrated into Visual Studio Code. The proposed approach combines (i) a locally hosted LLM, (ii) a curated vulnerability knowledge base, and (iii) IDE-level context extraction to support real-time detection and repair suggestions for JavaScript and TypeScript codebases. The system is evaluated against established rule-based SAST tools using reproducible benchmarks and quantitative metrics, including precision, recall, F1-score, latency, and resource usage.
