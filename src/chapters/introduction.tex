Modern software systems form the foundation of critical infrastructure, enterprise platforms, and consumer-facing applications. As software continues to increase in size, complexity, and development velocity, vulnerabilities introduced during implementation remain one of the dominant attack vectors. Industry and standards-oriented guidance repeatedly highlights recurring weakness classes—such as injection, broken access control, and insecure design—as persistent sources of exploitable flaws \cite{owaspTop10_2021,mitreCWEtop25,mitreCWE}. These weaknesses persist despite widespread adoption of secure development practices and automated analysis tools \cite{howard2006sdl,mcgraw2006softwaresecurity,nist800218}.

Static Application Security Testing (SAST) tools are commonly employed to detect vulnerabilities early in the Software Development Life Cycle (SDLC). Rule-based analyzers and taint-analysis-driven systems provide deterministic results and can scale to very large codebases \cite{bessey2010billion,chess2004staticanalysis,livshits2005java}. However, both empirical studies and practitioner experience highlight limitations, including false positives, difficulty capturing framework-specific semantics, and limited fix guidance \cite{johnson2013don,christakis2016developers}. As a result, developers frequently face large volumes of security findings that are costly to triage and may not reflect exploitable vulnerabilities.

In contemporary development workflows, security findings are often surfaced under significant time pressure. When a vulnerability is flagged, developers must interpret diagnostic output, locate the root cause, and apply an appropriate fix while preserving functional correctness. This process is cognitively demanding and error-prone, particularly for developers without specialized security expertise. Empirical studies show that warnings can be ignored, misinterpreted, or deprioritized when they are noisy or poorly explained \cite{johnson2013don,christakis2016developers}.

Recent advances in Large Language Models (LLMs) have demonstrated strong capabilities in source code understanding, generation, and transformation \cite{brown2020gpt3,chen2021codex,openai2023gpt4}. LLMs can produce plausible code completions and refactorings, enabling new forms of developer assistance directly in the IDE. However, empirical evaluations show that LLM-generated code can be \emph{functionally correct yet insecure}, and that models may hallucinate or omit security-critical constraints without explicit grounding \cite{pearce2022copilot,ji2023hallucination}.

Traditional SAST tools and LLM-based approaches exhibit complementary strengths and weaknesses. Static analyzers offer deterministic behavior and clear specifications but can struggle with noisy rule sets and missing semantic context. In contrast, LLMs provide flexible reasoning and explanation generation but are probabilistic and can hallucinate \cite{chess2004staticanalysis,ji2023hallucination}. This tension suggests that neither approach is sufficient in isolation for reliable vulnerability detection and repair.

Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to ground LLM outputs in external knowledge without retraining \cite{lewis2020rag,karpukhin2020dpr,guu2020realm,mialon2023augmented}. By augmenting prompts with retrieved vulnerability descriptions and secure coding guidance, RAG can reduce hallucinations and improve explanation consistency. Nevertheless, many existing assistants rely on cloud-hosted models or external services, raising privacy and confidentiality concerns when proprietary code is transmitted off-device. Privacy risks are amplified by known model leakage and memorization phenomena \cite{carlini2021extracting,gdpr2016}.

These concerns motivate the development of privacy-preserving, locally deployed LLM systems integrated directly into the Integrated Development Environment (IDE). Local deployment enables developers to benefit from advanced reasoning capabilities while preserving code confidentiality, reducing data leakage risks, and improving reproducibility. Despite increasing interest, the design and systematic evaluation of such systems—particularly those combining retrieval, static analysis, and LLM-based reasoning within the IDE—remain underexplored.

This thesis addresses this gap by investigating a privacy-preserving vulnerability detection and repair system based on retrieval-augmented local LLMs integrated into Visual Studio Code. The proposed approach combines (i) a locally hosted LLM, (ii) a curated vulnerability knowledge base, and (iii) IDE-level context extraction to support real-time detection and repair suggestions for JavaScript and TypeScript codebases. The system is evaluated against established rule-based SAST tools using reproducible benchmarks and quantitative metrics, including precision, recall, F1-score, latency, and resource usage.

\section{Objectives and Research Questions}
\label{sec:intro-objectives}

The overarching objective of this thesis is to design and evaluate an IDE-integrated secure coding assistant that improves developer feedback loops while preserving code confidentiality. Concretely, the thesis investigates the following research questions:
\begin{itemize}
  \item \textbf{RQ1 (Feasibility):} To what extent can a locally deployed LLM integrated into VS Code provide useful vulnerability detection and repair suggestions without transmitting source code to external services?
  \item \textbf{RQ2 (Grounding):} How does retrieval-augmented prompting influence detection quality, explanation consistency, and hallucination behavior compared to an LLM-only configuration?
  \item \textbf{RQ3 (Practicality):} What performance mechanisms (scoping, caching, debouncing) are necessary to make LLM-based security feedback usable within interactive IDE workflows?
\end{itemize}

\section{Scope and Assumptions}
\label{sec:intro-scope}

This thesis focuses on IDE-integrated security assistance for \textbf{JavaScript and TypeScript} codebases inside \textbf{Visual Studio Code}. The central privacy objective is \textbf{no source-code exfiltration}: analyzed code and IDE-derived context are sent only to a local LLM runtime. The system may optionally refresh its local security knowledge base from \emph{public} vulnerability metadata sources (e.g., CVE/NVD descriptions); this does not transmit user code and can be disabled for fully offline operation.

\subsection*{Threat Model and Trust Assumptions}

The primary threat addressed by this thesis is unintended disclosure of proprietary code through cloud-hosted analysis services. Local inference reduces this exposure surface, but does not remove all security risks.

\begin{table}[H]
  \centering
  \caption{Threat model summary for Code Guardian.}
  \label{tab:intro-threat-model}
  \footnotesize
  \renewcommand{\arraystretch}{1.15}
  \begin{tabularx}{\textwidth}{p{0.19\textwidth}p{0.28\textwidth}X}
    \toprule
    Threat class & Assumption / attack path & Design response in this thesis \\
    \midrule
    Source-code exfiltration & External API calls could expose proprietary code or derived context. & Local inference only; code and prompts sent to localhost backend; no code upload in evaluated workflows. \\
    Prompt injection & Attacker-controlled comments/strings try to override analysis instructions \cite{owaspLLMTop10_2023}. & Strict JSON-output contract in diagnostic mode; defensive parsing; human review before any repair is applied. \\
    Retrieval poisoning & Low-quality or malicious knowledge entries degrade grounding quality. & Use curated local knowledge sources and explicit retrieval scope; future work adds provenance scoring and stricter source controls. \\
    Local host compromise & Malware or untrusted local users can access code, prompts, or caches. & Out of scope for this thesis; assumes trusted developer machine and OS-level hardening. \\
    \bottomrule
  \end{tabularx}
\end{table}

In scope are privacy-preserving inference boundaries and robustness of developer-facing outputs. Out of scope are full endpoint hardening, hardware-level attacks, and enterprise identity/network controls.

\section{Contributions}
\label{sec:intro-contributions}

This work makes the following contributions:
\begin{itemize}
  \item \textbf{Code Guardian prototype:} a VS Code extension for local vulnerability analysis and developer-controlled repair suggestions for JavaScript/TypeScript.
  \item \textbf{Privacy-preserving architecture:} a design that keeps code and analysis artifacts on-device and uses optional local retrieval to ground model reasoning.
  \item \textbf{Evaluation harness:} a reproducible, repository-contained benchmark suite and script for comparing models and configurations using precision/recall/F1, structured-output robustness, and latency.
  \item \textbf{Implementation insights:} practical patterns for integrating local LLM inference into IDE feedback loops (debounced triggers, function-level scoping, and caching).
\end{itemize}

\section{Thesis Structure}
\label{sec:intro-structure}

Chapter~\ref{chap:analysis} derives requirements for privacy-preserving vulnerability detection and repair assistance. Chapter~\ref{chap:concept} presents the conceptual system design and its mapping to the requirements. Chapter~\ref{chap:implementation} details the implementation of Code Guardian as a VS Code extension with local inference and optional retrieval grounding. Chapter~\ref{chap:evaluation} evaluates detection quality, structured output robustness, and responsiveness. Chapter~\ref{chap:conclusion} concludes the thesis and Chapter~\ref{chap:future-work} outlines opportunities for further improvement.
