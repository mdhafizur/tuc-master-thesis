Privacy concerns pose a significant barrier to adopting LLM-based security tools in real-world settings. Cloud-hosted assistants require transmitting proprietary source code to external servers, which is unacceptable in many regulated or security-sensitive environments. Recent surveys on LLMs in cybersecurity emphasize the growing demand for on-premise and locally executed solutions \cite{kaur2025cyberreview,gholami2024llmcyber}.

IDE-integrated security tools have been shown to improve developer engagement and remediation rates by providing feedback during active development rather than post hoc analysis. However, most IDE-based LLM assistants prioritize productivity features such as code completion and refactoring, with limited focus on security or privacy guarantees. Existing research rarely evaluates latency, resource usage, and reproducibility in local deployment scenarios, despite these factors being critical for practical adoption.
