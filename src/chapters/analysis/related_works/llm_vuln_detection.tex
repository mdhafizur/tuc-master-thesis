Recent advances in Large Language Models (LLMs) have prompted extensive investigation into their use for vulnerability detection and secure code generation. Empirical studies demonstrate that LLMs can identify security flaws and generate syntactically correct code across a range of programming tasks \cite{fu2023llmsec}. However, early evaluations reveal that LLM-generated code is often \emph{functionally correct yet insecure}, containing subtle vulnerabilities that are not immediately apparent \cite{pearce2022copilot,peng2025cweval}.

Several approaches attempt to address these shortcomings through fine-tuning or constrained decoding. SafeCoder introduces security-aware fine-tuning to bias models toward safer outputs \cite{he2023safecoder}, while constrained decoding techniques enforce security constraints during generation \cite{li2024constrained}. Although effective in controlled settings, these methods require curated training data, retraining effort, or auxiliary security models, limiting their adaptability and suitability for local, resource-constrained deployment.

Other systems combine LLMs with static analyzers to mitigate hallucinations and improve reliability. LLM Security Guard integrates static analysis feedback into LLM reasoning to strengthen vulnerability detection \cite{kavian2024llmsecguard}. Similarly, IRIS employs LLMs to assist static analysis by interpreting and refining vulnerability reports \cite{li2025iris}. While these hybrid approaches improve detection quality, many rely on centralized or cloud-based execution and do not explicitly address privacy or IDE-level integration.
