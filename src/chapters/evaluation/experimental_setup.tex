\section{Experimental Setup}
\label{sec:eval-setup}

All experiments are executed locally using the Code Guardian evaluation harness in \texttt{code-guardian/evaluation/evaluate-models.js}. The harness iterates over the \emph{core} dataset described in Section~\ref{sec:eval-dataset} and invokes Ollama with a strict JSON-only system prompt. Model decoding is configured for low randomness (\texttt{temperature=0.1}) to improve determinism and parsing stability.

\subsection*{Response Schema for Scoring}

For evaluation purposes, the harness requests a richer structured output than the minimal in-editor diagnostics flow. In particular, each finding includes a vulnerability \texttt{type} string and a coarse \texttt{severity} level in addition to location and an optional fix. This enables type-level scoring (precision/recall) and per-category analysis. In the extension UI, vulnerability categories may appear within the message text and are used for downstream aggregation (e.g., in the workspace dashboard); a future refinement is to surface \texttt{type} and \texttt{severity} as first-class fields in diagnostics.

\subsection*{Compared Configurations}

Two configurations are relevant for the overall system design:
\begin{itemize}
  \item \textbf{LLM-only (quantitative):} the JSON-only analyzer evaluated by the harness.
  \item \textbf{LLM+RAG (qualitative / optional extension):} retrieval-augmented prompting using the local knowledge base and vector retrieval. This configuration is exercised in the prototypeâ€™s interactive workflows and can be evaluated quantitatively by extending the harness to inject retrieved snippets into the prompt.
\end{itemize}

Where relevant, results can also be stratified by model size or model family to study the trade-off between accuracy and latency.

\subsection*{Models and Hardware}

The prototype supports multiple local models (e.g., Qwen2.5-Coder, Gemma, and CodeLlama variants) via Ollama. The evaluation script contains a default list of models to test, but this list can be adjusted based on what is installed locally. For reproducibility, the reported results should include the exact model names, Ollama version, and hardware configuration (CPU/GPU and RAM).

\subsection*{Runtime Parameters}

The evaluation harness enforces a per-request timeout (default: 30\,s) and limits generation length (default: \texttt{num\_predict=1000}). A small delay is inserted between requests to reduce burstiness and minimize transient failures on developer hardware.

\paragraph{Response parsing.}
The harness strips Markdown code fences if present and then attempts to parse the remaining content as JSON. Responses that fail to parse are counted toward the parse failure rate and are scored as producing no issues, which impacts recall on vulnerable samples.

\subsection*{Running the Evaluation}

The evaluation can be reproduced by running:
\begin{lstlisting}[language=Java, caption={Running the Code Guardian evaluation harness}, label={lst:eval-run}]
cd code-guardian
node evaluation/evaluate-models.js
\end{lstlisting}

The script produces per-model metrics and can be extended to emit JSON/Markdown reports under \texttt{code-guardian/evaluation/logs/} for inclusion in the thesis appendix.
