\section{Qualitative Case Studies and Repair Suggestions}
\label{sec:eval-case-studies}

Quantitative metrics summarize overall detection behavior, but IDE usefulness also depends on explanation clarity and repair quality. This section reports qualitative observations from representative cases in the recorded ablation run.

\subsection*{Representative Cases}

\textbf{Case A (True Positive with direct remediation): SQL injection.}
Sample ID: \texttt{sql-injection-1}.
In the best-performing configuration (\texttt{qwen3:8b} with RAG), the model flagged direct string concatenation in the query and suggested parameterized queries. The suggested remediation aligned with the expected fix and was minimal.

\textbf{Case B (False Positive on secure sample): safe process invocation.}
Sample ID: secure snippet with \texttt{execFile} array arguments.
In \texttt{qwen3:8b} (LLM+RAG), this secure sample was still flagged, with a broad path-sanitization recommendation. This illustrates residual over-warning even in the strongest configuration.

\textbf{Case C (False Negative): NoSQL injection.}
Sample ID: NoSQL injection representative case.
In \texttt{qwen3:8b} (LLM+RAG), this vulnerable sample was missed in a representative run and no remediation suggestion was produced.

\textbf{Case D (True Positive): cross-site scripting via \texttt{innerHTML}.}
Sample ID: XSS representative case.
In \texttt{qwen3:8b} (LLM+RAG), the model correctly flagged unsafe HTML sink usage and proposed replacing \texttt{innerHTML} with safer rendering behavior.

\subsection*{Repair Suggestion Quality (R4)}

Repair suggestions are evaluated qualitatively against three criteria:
\begin{itemize}
  \item \textbf{Security adequacy:} does the fix mitigate the vulnerability class (e.g., parameterization for SQL injection)?
  \item \textbf{Minimality:} does it avoid unnecessary refactoring?
  \item \textbf{Applicability:} does it fit the code context and available APIs?
\end{itemize}

Because Code Guardian operates inside the IDE, repair suggestions are intentionally presented as \emph{optional} quick fixes, enabling developer review and preventing silent modifications.

\subsection*{Observed Failure Modes}

Observed failure modes in the run include:
\begin{itemize}
  \item \textbf{Parse instability:} some model/mode combinations return malformed or non-JSON outputs at high rates.
  \item \textbf{Over-warning:} secure patterns flagged as vulnerable, especially with \texttt{gemma3:4b}.
  \item \textbf{Under-warning:} conservative configurations (notably \texttt{gemma3:1b} with RAG) missing most vulnerable samples.
  \item \textbf{High-latency audit modes:} strongest F1 configurations may still be too slow for inline workflows.
\end{itemize}

\subsection*{Error Taxonomy from Run Logs}

\begin{table}[H]
  \centering
  \caption{Observed error taxonomy in the ablation run.}
  \label{tab:eval-error-taxonomy}
  \footnotesize
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{p{0.28\textwidth}p{0.30\textwidth}p{0.30\textwidth}}
    \toprule
    Error pattern & Representative evidence & Quantitative impact in this run \\
    \midrule
    Parse-collapse regimes & \texttt{qwen3:4b} and \texttt{CodeLlama:latest} (both modes) & Parse rates dropped to 1.01--5.05\%, and recall remained 0.00\% in all but one configuration (\texttt{CodeLlama:latest} LLM-only: 3.70\%). \\
    Secure-sample over-warning & \texttt{gemma3:4b} (LLM-only and LLM+RAG) & FPR reached 100\% in both modes (45/45 secure evaluations flagged). \\
    Conservative under-detection & \texttt{gemma3:1b} (especially LLM+RAG) & Recall was 5.56\% (3/54) in LLM-only and 0.00\% (0/54) with RAG. \\
    High-latency high-F1 mode & \texttt{qwen3:8b} (LLM+RAG) & Best F1 (42.70\%) and lower FPR (27.12\%) came with high latency (median 7809\,ms; mean 9012\,ms). \\
    \bottomrule
  \end{tabular}
\end{table}

These observations motivate the future work directions summarized in Chapter~\ref{chap:future-work}.
