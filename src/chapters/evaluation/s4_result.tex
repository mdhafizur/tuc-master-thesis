\section{Qualitative Case Studies and Repair Suggestions}
\label{sec:eval-case-studies}

Quantitative metrics summarize overall detection behavior, but IDE usefulness also depends on explanation clarity and repair quality. This section reports qualitative observations from representative cases.

\subsection*{Case Study Structure}

For each selected example, the analysis documents:
\begin{itemize}
  \item the vulnerable code fragment and its location,
  \item the reported vulnerability type and message,
  \item whether the finding is a true positive or false positive,
  \item the suggested fix (if present) and whether it addresses the root cause,
  \item any trade-offs or functional impacts introduced by the fix.
\end{itemize}

\subsection*{Repair Suggestion Quality (R4)}

Repair suggestions are evaluated qualitatively against three criteria:
\begin{itemize}
  \item \textbf{Security adequacy:} does the fix mitigate the vulnerability class (e.g., parameterization for SQL injection)?
  \item \textbf{Minimality:} does it avoid unnecessary refactoring?
  \item \textbf{Applicability:} does it fit the code context and available APIs?
\end{itemize}

Because Code Guardian operates inside the IDE, repair suggestions are intentionally presented as \emph{optional} quick fixes, enabling developer review and preventing silent modifications.

\subsection*{Observed Failure Modes}

Common failure modes to track include:
\begin{itemize}
  \item \textbf{Over-warning:} benign patterns flagged without sufficient evidence (precision loss).
  \item \textbf{Under-warning:} vulnerabilities missed when the relevant context lies outside the analyzed scope (recall loss).
  \item \textbf{Inexact localization:} correct type but wrong line range.
  \item \textbf{Overconfident fixes:} suggested patch changes behavior beyond security hardening.
\end{itemize}

These observations motivate the future work directions summarized in Chapter~\ref{chap:future-work}.
