\section{Results â€“ LangExtract Baseline (S5)}
\label{sec:eval-langextract}

\textbf{LangExtract} is an open-source, LLM-powered Python library for schema-constrained extraction. In this work it is used as an external baseline rather than a core component of the proposed system: it provides a deterministic orchestration wrapper around model calls (strict JSON schema enforcement and span grounding), but is not integrated into the multi-agent architecture.

On the MUC-4 subset ($N{=}100$ documents), LangExtract achieves an overall balanced score (OBS) of $0.684$ and a format-driven accuracy (FDA) of $0.756$, with a relatively low hallucination rate (HR) of $0.088$ and a high rate of fully correct fields (RFA $=0.796$) (see Table~\ref{tab:langextract-headline} in Appendix~\ref{appendix:detailed-results}). This pattern indicates a conservative filler: when the system commits to a value it is usually accurate, but it frequently abstains. The non-empty specific metrics reflect this behaviour: the non-empty $F_1$ score remains modest ($0.406$), and the non-empty exact accuracy (EAI) of $0.182$ shows that much of the performance comes from correctly leaving fields empty when the gold label is empty.

Field-wise analysis (Table~\ref{tab:langextract-perfield}) reveals a skew towards easier categorical slots such as \texttt{incidentType}, \texttt{weapon}, and \texttt{incidentStage}, which achieve comparatively high scores, whereas more complex or ambiguous fields such as \texttt{target}, \texttt{incidentLocation}, and especially \texttt{perpetratorOrganization} perform noticeably worse under gold-nonempty evaluation. The \texttt{incidentDate} field is particularly under-filled: in cases where the gold label is nonempty, the model often abstains, contributing to the higher miss rate (MR $=0.369$).

The embedding-based metrics in Table~\ref{tab:langextract-embed-overall} and Table~\ref{tab:langextract-embed-perfield} confirm that LangExtract produces stylistically consistent outputs with reasonable semantic coverage: soft coverage and specificity are balanced, and the symmetric Chamfer scores are stable across text and list fields. Consistency metrics (schema format vs.\ style, see Appendix~\ref{appendix:detailed-implementation}) show perfect schema adherence ($\mathrm{FPR}_{\text{overall}}=1.000$) and a style consistency score of $\mathrm{SC}_{\text{macro}}=0.709$, which is comparable to the best multi-agent strategies.

In terms of runtime, LangExtract exhibits moderate latency, with a median of approximately $21$\,s per document and a long tail extending to around $110$\,s (Table~\ref{tab:langextract-latency}). This makes it practical as a batch or asynchronous baseline, but less attractive as an interactive component. Overall, S5 serves as a useful external point of comparison: it demonstrates that a mature off-the-shelf extractor can reach solid accuracy under conservative filling, but it does not outperform the best multi-agent strategies in the non-empty and robustness-oriented metrics that are central to this thesis.
