\section{Evaluation Metrics}
\label{sec:eval-metrics}

We evaluate Code Guardian along complementary axes aligned with Chapter~\ref{chap:analysis}: detection quality (R1--R2), explainability/structure (R3), and responsiveness (R6).

\subsection*{Detection Quality}

Given an expected set of vulnerability types per test case and a detected set of vulnerability types returned by the model, we compute:
\begin{itemize}
  \item \textbf{Precision:} $\mathrm{TP}/(\mathrm{TP}+\mathrm{FP})$
  \item \textbf{Recall:} $\mathrm{TP}/(\mathrm{TP}+\mathrm{FN})$
  \item \textbf{F1 score:} harmonic mean of precision and recall
  \item \textbf{False positive rate (FPR):} $\mathrm{FP}/(\mathrm{FP}+\mathrm{TN})$ on secure examples
  \item \textbf{Accuracy:} $(\mathrm{TP}+\mathrm{TN})/N$
\end{itemize}

Matching is performed at the vulnerability-type level. To account for naming variation (e.g., ``Cross-Site Scripting'' vs.\ ``XSS''), the evaluation script applies case-insensitive substring matching between expected and detected type strings.

\paragraph{Secure examples.}
The datasets include a small number of intentionally secure snippets (core: 2; advanced: 3). These examples are used to estimate the false positive rate and to sanity-check whether a model tends to over-warn under strict JSON output constraints.

\subsection*{Structured Output Robustness}

Because Code Guardian integrates findings into IDE diagnostics, structured output is essential. We therefore report:
\begin{itemize}
  \item \textbf{JSON parse success rate:} percentage of responses that parse into a JSON array of issues.
\end{itemize}

\paragraph{How parse failures are treated.}
In the evaluation harness, responses that do not parse as a JSON array are counted as parse failures and are scored as producing an empty set of issues. For vulnerable test cases, this manifests as false negatives (missed findings); for secure test cases, it can appear as a true negative but still indicates that the system is not usable for IDE automation. Reporting parse success rate separately is therefore important to avoid over-interpreting accuracy numbers when a model frequently produces malformed output.

\subsection*{Responsiveness}

We measure:
\begin{itemize}
  \item \textbf{Response time:} wall-clock time per analysis call (milliseconds), summarized by mean and median.
\end{itemize}

Latency is interpreted in the context of IDE workflows: function-level analysis has tighter budgets than file-level or workspace scans.

\paragraph{Limits of the scoring setup.}
The current quantitative scoring focuses on vulnerability-type detection rather than exact localization quality. This is a deliberate choice aligned with the harness design (type strings and severities are available for scoring), but it means that line-range accuracy and patch minimality are evaluated qualitatively rather than through automated metrics.
