\section{Results: LLM-Only Configuration}
\label{sec:eval-llm-only}

This section reports results for Code Guardian when operating without retrieval augmentation. The goal is to establish a baseline for detection quality and latency when the model receives only the analyzed code and strict JSON-output constraints.

\subsection*{Detection Quality}

Table~\ref{tab:eval-llm-only} summarizes precision, recall, and F1 score on the curated dataset. These values should be reported per model, as different local models can show distinct precision/recall trade-offs.

\begin{table}[H]
  \centering
  \caption{LLM-only evaluation metrics on the curated dataset.}
  \label{tab:eval-llm-only}
  \small
  \begin{tabular}{lccccc}
    \toprule
    Model & Precision (\%) & Recall (\%) & F1 (\%) & FPR (\%) & Parse rate (\%) \\
    \midrule
    \texttt{gemma3:1b} & 100.00 & 5.56 & 10.53 & 0.00 & 100.00 \\
    \texttt{qwen3-coder} & 19.05 & 44.44 & 26.67 & 100.00 & 100.00 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Latency}

For interactive usage, response time is critical. Table~\ref{tab:eval-llm-only-latency} reports median and mean latency per analysis call for each model under the evaluation harness.

\begin{table}[H]
  \centering
  \caption{LLM-only latency metrics.}
  \label{tab:eval-llm-only-latency}
  \small
  \begin{tabular}{lcc}
    \toprule
    Model & Median (ms) & Mean (ms) \\
    \midrule
    \texttt{gemma3:1b} & 184 & 207 \\
    \texttt{qwen3-coder} & 1099 & 1494 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Discussion}

The LLM-only results show a clear trade-off between precision, recall, and latency. \texttt{gemma3:1b} is very fast and conservative (high precision, very low recall), while \texttt{qwen3-coder} detects more vulnerable samples but over-predicts on secure samples (FPR 100\%) and is substantially slower. Both models maintained 100\% JSON parse success.
