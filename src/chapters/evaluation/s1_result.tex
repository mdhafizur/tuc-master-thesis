\section{Results: LLM-Only Configuration}
\label{sec:eval-llm-only}

This section reports results for Code Guardian when operating without retrieval augmentation. The goal is to establish a baseline for detection quality and latency when the model receives only the analyzed code and strict JSON-output constraints.

\subsection*{Detection Quality}

Table~\ref{tab:eval-llm-only} summarizes precision, recall, and F1 score on the curated dataset. These values should be reported per model, as different local models can show distinct precision/recall trade-offs.

\begin{table}[H]
  \centering
  \caption{LLM-only evaluation metrics on the curated dataset (fill with measured values).}
  \label{tab:eval-llm-only}
  \begin{tabular}{lccccc}
    \toprule
    Model & Precision (\%) & Recall (\%) & F1 (\%) & FPR (\%) & Parse rate (\%) \\
    \midrule
    \texttt{<model-1>} & -- & -- & -- & -- & -- \\
    \texttt{<model-2>} & -- & -- & -- & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Latency}

For interactive usage, response time is critical. Table~\ref{tab:eval-llm-only-latency} reports median and mean latency per analysis call for each model under the evaluation harness.

\begin{table}[H]
  \centering
  \caption{LLM-only latency metrics (fill with measured values).}
  \label{tab:eval-llm-only-latency}
  \begin{tabular}{lcc}
    \toprule
    Model & Median (ms) & Mean (ms) \\
    \midrule
    \texttt{<model-1>} & -- & -- \\
    \texttt{<model-2>} & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Discussion}

The LLM-only configuration is typically the lowest-latency option and is therefore suitable for real-time feedback in the editor. However, without grounding, it can be more sensitive to hallucinations and may produce less consistent classifications for borderline patterns. The next section evaluates whether retrieval augmentation improves grounding and reduces false positives.
