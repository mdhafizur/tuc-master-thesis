\section{Model Comparison and Category Breakdown}
\label{sec:eval-models}

Code Guardian supports multiple local models through Ollama. In practice, model choice affects not only detection quality but also structured-output robustness and latency. This section outlines how results can be analyzed by model and by vulnerability category.

\subsection*{Model Ranking}

Using the evaluation harness, models can be ranked by F1 score subject to a minimum JSON parse success threshold (e.g., $\geq 90\%$). This avoids selecting a high-F1 model that frequently produces malformed output, which would harm IDE integration.

\subsection*{Per-Category Performance}

Different vulnerability types can exhibit different error profiles. For example, injection sinks are often easy to detect but harder to localize precisely; configuration-level issues (e.g., CORS) can be detected but tend to yield higher false positives without context. Table~\ref{tab:eval-per-category} provides a template for reporting category-wise results.

\begin{table}[H]
  \centering
  \caption{Per-category performance (fill with measured values).}
  \label{tab:eval-per-category}
  \begin{tabular}{lccc}
    \toprule
    Category & Precision (\%) & Recall (\%) & F1 (\%) \\
    \midrule
    Injection (SQL/XSS/Command) & -- & -- & -- \\
    Path traversal / file ops & -- & -- & -- \\
    Authentication / authorization & -- & -- & -- \\
    Cryptography / randomness & -- & -- & -- \\
    Web security (CORS/CSRF/Open redirect) & -- & -- & -- \\
    Other & -- & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Interpretation}

This breakdown supports targeted improvements: categories with low precision motivate tighter prompting and better abstention behavior; categories with low recall motivate broader context extraction and stronger retrieval coverage. These findings also inform the selection of different default models for real-time vs.\ audit usage.
