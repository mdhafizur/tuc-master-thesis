\section{Model Comparison and Category Breakdown}
\label{sec:eval-models}

Code Guardian supports multiple local models through Ollama. In practice, model choice affects not only detection quality but also structured-output robustness and latency. This section summarizes the measured ablation results and category-level behavior observed in the run logs.

\subsection*{Ablation Comparison}

Table~\ref{tab:eval-ablation-summary} compares all measured model/prompt-mode combinations.

\begin{table}[H]
  \centering
  \caption{Ablation summary across model and prompt mode.}
  \label{tab:eval-ablation-summary}
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{lcccccc}
      \toprule
      Configuration & Precision (\%) & Recall (\%) & F1 (\%) & FPR (\%) & Parse (\%) & Median (ms) \\
      \midrule
      \texttt{qwen3:8b} (LLM+RAG) & 54.29 & 35.19 & 42.70 & 27.12 & 84.85 & 7809 \\
      \texttt{qwen3:8b} (LLM-only) & 38.30 & 33.33 & 35.64 & 39.73 & 77.78 & 8774 \\
      \texttt{gemma3:4b} (LLM+RAG) & 24.24 & 44.44 & 31.37 & 100.00 & 100.00 & 1335 \\
      \texttt{gemma3:4b} (LLM-only) & 23.96 & 42.59 & 30.67 & 100.00 & 96.97 & 1415 \\
      \texttt{gemma3:1b} (LLM-only) & 100.00 & 5.56 & 10.53 & 0.00 & 100.00 & 178 \\
      \texttt{CodeLlama:latest} (LLM-only) & 40.00 & 3.70 & 6.78 & 6.25 & 5.05 & 3987 \\
      \texttt{gemma3:1b} (LLM+RAG) & 0.00 & 0.00 & 0.00 & 0.00 & 100.00 & 180 \\
      \texttt{qwen3:4b} (LLM-only) & 0.00 & 0.00 & 0.00 & 0.00 & 1.01 & 9549 \\
      \texttt{qwen3:4b} (LLM+RAG) & 0.00 & 0.00 & 0.00 & 0.00 & 4.04 & 9852 \\
      \texttt{CodeLlama:latest} (LLM+RAG) & 0.00 & 0.00 & 0.00 & 10.00 & 1.01 & 5478 \\
      \bottomrule
    \end{tabular}%
  }
\end{table}

\subsection*{Per-Category Observations}

Based on detailed run logs, injection-style cases (SQL injection and XSS) were consistently detected by \texttt{qwen3:8b} in the best-performing configuration. At the same time, representative misses remained (e.g., NoSQL injection), and secure samples still produced false alarms in several configurations. The most extreme alert-noise behavior appears in \texttt{gemma3:4b}, where secure evaluations were flagged in every run (FPR 100\% in both modes).

\subsection*{Interpretation}

These results motivate two practical follow-ups: stronger output-contract enforcement for models with low parse reliability, and stricter abstention behavior on secure samples for high-noise configurations. They also support differentiated defaults: a fast conservative model for inline checks and a higher-accuracy model for audit mode when higher latency is acceptable.
