\section{Results: LLM+RAG Configuration}
\label{sec:eval-rag}

This section discusses Code Guardian with retrieval augmentation enabled. In this configuration, the prompt is enriched with locally retrieved security knowledge (CWE/OWASP/CVE-derived summaries and mitigation guidance) to ground the model’s output. Quantitative scoring of this configuration requires extending the evaluation harness to inject retrieved snippets into the prompt; the tables below provide a reporting template once such runs are produced.

\subsection*{Detection Quality}

Table~\ref{tab:eval-rag} summarizes detection metrics for the RAG-enhanced configuration. Comparing Table~\ref{tab:eval-rag} against Table~\ref{tab:eval-llm-only} isolates the empirical impact of retrieval augmentation on precision/recall trade-offs.

\begin{table}[H]
  \centering
  \caption{LLM+RAG evaluation metrics on the curated dataset (fill with measured values).}
  \label{tab:eval-rag}
  \begin{tabular}{lccccc}
    \toprule
    Model & Precision (\%) & Recall (\%) & F1 (\%) & FPR (\%) & Parse rate (\%) \\
    \midrule
    \texttt{<model-1>} & -- & -- & -- & -- & -- \\
    \texttt{<model-2>} & -- & -- & -- & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Latency Overhead}

Retrieval introduces overhead from embedding, vector search, and prompt expansion. Table~\ref{tab:eval-rag-latency} reports the latency impact of enabling RAG under the same evaluation harness.

\begin{table}[H]
  \centering
  \caption{LLM+RAG latency metrics (fill with measured values).}
  \label{tab:eval-rag-latency}
  \begin{tabular}{lcc}
    \toprule
    Model & Median (ms) & Mean (ms) \\
    \midrule
    \texttt{<model-1>} & -- & -- \\
    \texttt{<model-2>} & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Discussion}

RAG is expected to improve grounding and reduce unsupported claims, particularly for vulnerability categories that benefit from precise definitions and mitigation checklists. In practice, the impact depends on retrieval quality (knowledge coverage and relevance) and on the model’s ability to incorporate retrieved context without overfitting to irrelevant snippets. The qualitative case studies in Section~\ref{sec:eval-case-studies} illustrate typical improvements and failure modes.
