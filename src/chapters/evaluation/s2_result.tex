\section{Results: LLM+RAG Configuration}
\label{sec:eval-rag}

This section reports Code Guardian with retrieval augmentation enabled. In this configuration, the prompt is enriched with locally retrieved security knowledge (CWE/OWASP/CVE-derived summaries and mitigation guidance) to ground the modelâ€™s output.

\subsection*{Detection Quality}

Table~\ref{tab:eval-rag} summarizes detection metrics for the RAG-enhanced configuration. Comparing Table~\ref{tab:eval-rag} against Table~\ref{tab:eval-llm-only} isolates the empirical impact of retrieval augmentation on precision/recall trade-offs.

\begin{table}[H]
  \centering
  \caption{LLM+RAG evaluation metrics on the curated dataset.}
  \label{tab:eval-rag}
  \small
  \begin{tabular}{lccccc}
    \toprule
    Model & Precision (\%) & Recall (\%) & F1 (\%) & FPR (\%) & Parse rate (\%) \\
    \midrule
    \texttt{gemma3:1b} & 0.00 & 0.00 & 0.00 & 0.00 & 100.00 \\
    \texttt{qwen3-coder} & 27.27 & 55.56 & 36.59 & 100.00 & 100.00 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Latency Overhead}

Retrieval introduces overhead from embedding, vector search, and prompt expansion. Table~\ref{tab:eval-rag-latency} reports the latency impact of enabling RAG under the same evaluation harness.

\begin{table}[H]
  \centering
  \caption{LLM+RAG latency metrics.}
  \label{tab:eval-rag-latency}
  \small
  \begin{tabular}{lcc}
    \toprule
    Model & Median (ms) & Mean (ms) \\
    \midrule
    \texttt{gemma3:1b} & 182 & 185 \\
    \texttt{qwen3-coder} & 1060 & 1140 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Discussion}

RAG effects are model-dependent in this run. For \texttt{qwen3-coder}, RAG improved precision, recall, and F1 while also reducing latency compared to its LLM-only configuration. For \texttt{gemma3:1b}, RAG collapsed recall to zero and produced no true positives. This indicates that retrieval augmentation improved performance only when the base model could effectively incorporate retrieved context.
