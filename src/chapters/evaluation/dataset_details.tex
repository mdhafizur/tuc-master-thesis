\section{Dataset and Preprocessing}
\label{sec:eval-dataset}

The \textbf{MUC-4 dataset} was introduced as part of the Message Understanding Conference (MUC) evaluation series and remains one of the most established benchmarks for information extraction in the terrorism domain. It consists of English-language newswire articles in which domain experts annotated terrorist incidents that occurred primarily in Latin America during the late 1980s. Each article may describe one or more events, and each event is represented by a structured template containing up to \textbf{24 annotated fields} that capture various attributes of the incident, including participants, timing, and contextual information.

For the purpose of this evaluation, the full schema was reduced to the subset of fields that are both semantically informative and consistently populated. The final evaluation schema includes: \texttt{incidentType}, \texttt{incidentDate}, \texttt{incidentLocation}, \texttt{incidentStage}, \texttt{perpetratorIndividual}, \texttt{perpetratorOrganization}, \texttt{target}, \texttt{victim}, and \texttt{weapon}. These correspond directly to the slots defined in the \textit{Invox} template-filling system and reflect the most important aspects of event understanding. This reduction enables a focused evaluation of the system’s ability to extract the key components of each incident rather than peripheral metadata.

Each article in MUC-4 can contain multiple annotated events. To align with the design of the \textit{Invox} pipeline—where each input corresponds to a single structured template—the evaluation considers only one event per document. When multiple event templates are available, the preprocessing procedure automatically selects the most complete annotation, typically the first one with non-empty values across the selected fields. This ensures a one-to-one correspondence between document text and evaluated template, which simplifies both comparison and interpretation.

\subsection{Preprocessing}

The preprocessing pipeline is applied exclusively to the \textbf{gold annotations} provided by the MUC-4 dataset, as the system outputs generated by \textit{Invox} already conform to the standardized schema. The original MUC-4 corpus is distributed in two complementary formats: 
(1) \textit{value files}, which contain the full article texts along with document identifiers, and 
(2) \textit{key files}, which contain the corresponding event templates with up to 24 annotated fields for each document. 
To construct a consistent evaluation dataset, these two sources were combined into a unified JSON representation in which each document entry contains both the raw text and the corresponding gold template.

Once the text–template pairs were combined, a sequence of normalization and filtering steps was applied to clean and harmonize the annotations:

\begin{itemize}
    \item \textbf{Schema normalization:} all gold templates were converted into a unified JSON structure using the standardized field names: \texttt{incidentType}, \texttt{incidentDate}, \texttt{incidentLocation}, \texttt{incidentStage}, \texttt{perpetratorIndividual}, \texttt{victim}, \texttt{target}, \texttt{perpetratorOrganization},  and \texttt{weapon}. Fields outside this subset were discarded.
    \item \textbf{Event filtering:} each MUC-4 document can include multiple annotated events. To ensure compatibility with the \textit{Invox} workflow, where each input corresponds to a single structured template, only one event per document was retained. The selection heuristic prioritized the event with the most complete slot coverage (i.e., the greatest number of non-empty fields).
    \item \textbf{Text normalization:} all article texts were lowercased, punctuation standardized, and redundant whitespace removed to facilitate reliable tokenization and embedding consistency during evaluation.
    \item \textbf{Placeholder cleaning:} annotation placeholders such as ``--'', ``N/A'', or empty markers were removed from the gold templates to prevent artificial mismatches during similarity computation.
    \item \textbf{Deduplication of fillers:} repeated mentions of the same entity within a field (for example, a perpetrator organization listed multiple times) were collapsed into unique entries to avoid double counting.
\end{itemize}

After these operations, each document was represented as a normalized JSON object containing one cleaned article text and a single corresponding event template. The processed dataset includes approximately \textbf{1,300 examples} derived from the original MUC-4 training and development files, which were used to populate the retrieval index and provide exemplars for the RAG module. A separate subset of \textbf{100 held-out examples} was reserved for evaluation and served as the test set for measuring extraction quality. 

This preprocessing ensures a harmonized dataset structure that faithfully preserves the original semantic content while enabling consistent embedding-based comparison between gold annotations and the structured templates generated by the \textit{Invox} system.


\subsection{Schema Caveats for Slot Filling}

While MUC-4 is a classic benchmark, its slot schema introduces mismatches that matter in practice. A single article often mentions several incidents, yet the template expects one event, forcing systems to choose which facts to keep and which to drop. Roles can be underspecified or mixed—generic actors like “rebels” appear alongside named groups and people—so the split between \texttt{perpetratorIndividual} and \texttt{perpetratorOrganization} is frequently ambiguous. Granularity also varies by field: locations oscillate between country–city–neighborhood and just a country; weapons range from specific (“car bomb”) to coarse (“explosive”) or even dashes; dates are sometimes relative in text (“yesterday”) but absolute in gold. These inconsistencies create false errors for otherwise sensible fills.

A second issue is calibration and evidence. Gold templates sometimes omit entities that are clearly in the text, and at other times include fillers that are only implied (e.g., an organization named via allegation). This rewards systems that guess and penalizes ones that abstain without confirmation. Combined with translation/OCR artifacts (casing, accents, bracketed notes), exact-match scoring can overstate mistakes. Our setup mitigates this with normalization, soft/semantic matching, and an explicit fill-vs-abstain decision, but these schema quirks remain important when interpreting scores and designing post-processing.
