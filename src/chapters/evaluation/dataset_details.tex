\section{Datasets}
\label{sec:eval-dataset}

The evaluation datasets are curated sets of JavaScript and TypeScript vulnerability test cases maintained alongside the Code Guardian prototype in \texttt{code-guardian/evaluation/datasets/}. Each test case consists of (i) a short code snippet, (ii) a list of expected vulnerabilities with CWE identifiers and severities, and (iii) an expected remediation description. Two datasets are provided:
\begin{itemize}
  \item \textbf{Core dataset:} \texttt{vulnerability-test-cases.json} contains representative examples for common vulnerability classes (e.g., SQL injection, XSS, command injection, path traversal) as well as a small number of secure examples to measure false positives.
  \item \textbf{Advanced dataset:} \texttt{advanced-test-cases.json} contains additional vulnerability types and more nuanced patterns (e.g., insecure CORS configuration, timing attacks, mass assignment).
\end{itemize}

In the current prototype version, the core dataset contains \textbf{20} test cases (\textbf{18} vulnerable and \textbf{2} secure), and the advanced dataset contains \textbf{28} test cases (\textbf{25} vulnerable and \textbf{3} secure). Expected findings in both datasets are annotated with CWE identifiers to support aggregation by vulnerability class.

\paragraph{Which dataset is scored by default.}
The repository-contained evaluation harness used in this thesis (\texttt{code-guardian/evaluation/evaluate-models.js}) loads and scores the \emph{core} dataset by default. The advanced dataset is included to broaden coverage and can be evaluated by extending the harness to load an additional JSON file or to run two passes (core vs.\ advanced). Unless stated otherwise, quantitative results in Chapter~\ref{chap:evaluation} refer to the core dataset.

Vulnerability classes are aligned with the Common Weakness Enumeration taxonomy \cite{mitreCWE}, and severities are recorded as coarse labels to support prioritization and reporting. Where applicable, severity can be related back to standardized scoring systems such as CVSS and public vulnerability databases such as CVE/NVD \cite{firstCvss31,mitreCVE,nistNVD}.

This dataset targets requirement R2 (context-aware vulnerability reasoning) by including vulnerability instances that cannot be resolved purely by superficial keyword matching (e.g., sink usage that requires reasoning about tainted input). It also supports R1 by enabling reproducible comparisons across repeated runs and across different local models.

\subsection*{Dataset Schema}

Each record follows a uniform schema:
\begin{itemize}
  \item \texttt{id}: unique identifier
  \item \texttt{name}: descriptive test name
  \item \texttt{code}: code snippet under test
  \item \texttt{expectedVulnerabilities}: list of expected findings (type, CWE, severity, description)
  \item \texttt{expectedFix}: short remediation guidance (optional)
\end{itemize}

The dataset is intentionally small and human-auditable to facilitate iteration on prompts, retrieval, and output validation. Limitations of synthetic snippets and representativeness are discussed in Section~\ref{sec:eval-summary}.

\paragraph{Toward larger benchmarks.}
While this thesis focuses on a curated, auditable suite to support rapid iteration, standard benchmark suites such as the OWASP Benchmark and NIST Juliet can be used to broaden coverage and stress-test generalization in future work \cite{owaspBenchmark,nistJuliet}. In addition, secure coding checklists and verification frameworks such as OWASP ASVS can guide the selection of security requirements and test categories when scaling the evaluation \cite{owaspASVS}.
