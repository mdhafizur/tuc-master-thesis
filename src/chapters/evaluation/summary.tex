\section{Summary and Discussion}
\label{sec:eval-summary}

The evaluation framework provides reproducible measurements of detection quality, structured output robustness, and latency for Code Guardian. The curated dataset enables rapid iteration on prompts, retrieval, and output validation, while the model-comparison view supports selecting an appropriate default model for real-time IDE use.

\subsection*{Key Takeaways}

\begin{itemize}
  \item \textbf{R1--R2 (quality and consistency):} Precision/recall trade-offs vary across models; RAG can improve grounding but may introduce latency overhead.
  \item \textbf{R3 (explainability):} IDE-native rendering (diagnostics, hovers) keeps explanations close to code, but message quality depends on prompt design and knowledge coverage.
  \item \textbf{R4 (repairs):} Quick fixes are effective as an assistive mechanism when suggestions are minimal and context-appropriate; developer confirmation remains essential.
  \item \textbf{R5 (privacy):} All source code analysis and model inference remain local; only public vulnerability metadata may be fetched for knowledge updates.
  \item \textbf{R6 (responsiveness):} Debouncing and caching are critical to keep real-time analysis practical under normal editing patterns.
\end{itemize}

\subsection*{Limitations}

The dataset is intentionally small and synthetic; therefore, it may not capture the full diversity of real-world codebases (framework-specific patterns, multi-file flows, and complex validation logic). Future work should include evaluation on larger benchmarks and real repositories, as discussed in Chapter~\ref{chap:future-work}.

\subsection*{Threats to Validity}

\textbf{Internal validity.} The evaluation harness scores detections at the vulnerability-type level using substring matching between expected and predicted category names. This reduces brittleness to naming variation but can also over-credit partially correct labels (e.g., a broad ``Injection'' label matching multiple injection subclasses). A stricter label ontology or CWE-level normalization would improve precision of the evaluation itself.

\textbf{Construct validity.} Precision/recall and parse success rate measure important properties for IDE integration, but they do not fully capture developer value. In practice, usefulness also depends on (i) localization quality (correct lines), (ii) explanation clarity, and (iii) the security adequacy and minimality of suggested fixes. These are partially addressed through the qualitative case-study structure (Section~\ref{sec:eval-case-studies}) but are not yet quantified.

\textbf{External validity.} Synthetic snippets are easier than real codebases with frameworks, configuration files, and cross-file flows. Results on the curated datasets should therefore be interpreted as an estimate of core capability under controlled conditions rather than a definitive measure of real-world performance. The most likely failure mode in practice is missing context when only a small scope (e.g., a single function) is analyzed.

\subsection*{Reproducibility Notes}

To support reproducibility, the benchmark suite and evaluation harness are kept in the same repository as the prototype. For comparable results across machines, the reported numbers should include the Ollama version, the selected model(s), and hardware characteristics. Where possible, evaluation should be repeated across multiple runs to estimate variability due to runtime effects (warm-up, caching, and transient system load) even under low-temperature decoding.
