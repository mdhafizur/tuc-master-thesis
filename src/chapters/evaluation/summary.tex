\section{Summary and Discussion}
\label{sec:eval-summary}

The evaluation framework provides reproducible measurements of detection quality, structured output robustness, and latency for Code Guardian. The curated dataset enables rapid iteration on prompts, retrieval, and output validation, while the model-comparison view supports selecting an appropriate default model for real-time IDE use.

\subsection*{Key Takeaways}

\begin{itemize}
  \item \textbf{R1--R2 (quality and consistency):} Model behavior differs strongly. In this run, \texttt{qwen3:8b} (LLM+RAG) achieved the highest F1 (42.70\%) with lower FPR (27.12\%) than other high-recall configurations, while \texttt{gemma3:1b} remained highly conservative (precision 100.00\%, recall 5.56\%).
  \item \textbf{RAG impact is model-dependent:} RAG improved \texttt{qwen3:8b} (F1: 35.64\% $\rightarrow$ 42.70\%; median latency: 8774 $\rightarrow$ 7809 ms) and slightly improved \texttt{gemma3:4b} (F1: 30.67\% $\rightarrow$ 31.37\%), but reduced \texttt{gemma3:1b} recall to zero. \texttt{qwen3:4b} and \texttt{CodeLlama:latest} remained ineffective due to parse instability.
  \item \textbf{R3 (explainability):} IDE-native rendering (diagnostics, hovers) keeps explanations close to code, but message quality depends on prompt design and knowledge coverage.
  \item \textbf{R4 (repairs):} Quick fixes are effective as an assistive mechanism when suggestions are minimal and context-appropriate; developer confirmation remains essential.
  \item \textbf{R5 (privacy):} All source code analysis and model inference remain local; only public vulnerability metadata may be fetched for knowledge updates.
  \item \textbf{R6 (responsiveness):} Responsiveness depends heavily on model profile (about 178--180 ms median for \texttt{gemma3:1b}, about 1.3--1.4 s for \texttt{gemma3:4b}, and about 7.8--9.9 s for \texttt{qwen3} variants).
\end{itemize}

\subsection*{Claim-to-Evidence Map}

Table~\ref{tab:claim-evidence-map} links the thesis research questions to the concrete measurements and sections used as evidence.

\begin{table}[H]
  \centering
  \caption{Claim-to-evidence map for core research questions.}
  \label{tab:claim-evidence-map}
  \footnotesize
  \renewcommand{\arraystretch}{1.15}
  \begin{tabularx}{\textwidth}{p{0.12\textwidth}p{0.30\textwidth}p{0.28\textwidth}X}
    \toprule
    RQ & Claim tested & Evidence used & Outcome in this thesis run \\
    \midrule
    RQ1 (Feasibility) & Local IDE assistant can provide useful security findings without source-code exfiltration. & Privacy boundary and local deployment design (Chapter~\ref{chap:concept}); detection/latency measurements (Sections~\ref{sec:eval-llm-only}--\ref{sec:eval-models}). & Supported with caveats: useful findings are produced locally, but model choice strongly affects recall and false positives. \\
    RQ2 (Grounding) & Retrieval augmentation improves quality and consistency versus LLM-only. & Ablation results across prompt modes and models (Section~\ref{sec:eval-models}); descriptive mode-to-mode comparison in this section. & Partially supported and model-dependent: clear gains for \texttt{qwen3:8b}, marginal gain for \texttt{gemma3:4b}, degradation for \texttt{gemma3:1b}. \\
    RQ3 (Practicality) & Real-time IDE use is achievable with scoping/caching/debouncing. & Runtime design and guardrails (Chapter~\ref{chap:implementation}); measured latency distributions (Sections~\ref{sec:eval-llm-only}, \ref{sec:eval-rag}). & Supported for small models and constrained workflows; high-performing large models remain primarily audit-mode due to multi-second latency. \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsection*{Run Variability (Descriptive)}

Each configuration was evaluated three times per sample. Reported percentages are run-level descriptive values over pooled evaluations (e.g., recall over 54 vulnerable evaluations and parse rate over 99 total requests). Because repeated runs reuse the same snippets and paired per-sample mode outputs were not exported by the harness, this thesis reports directional mode-to-mode differences descriptively and does not make inferential claims.

Latency was measured across repeated calls per sample. For high-latency configurations (\texttt{qwen3:8b}, \texttt{qwen3:4b}, and \texttt{CodeLlama:latest}), mean latency remained above median in both modes, indicating right-skewed response-time tails under local inference.

\subsection*{Mode-to-Mode Recall Differences (Descriptive)}

Table~\ref{tab:eval-recall-delta} summarizes recall changes between LLM-only and LLM+RAG for each model in percentage points.

\begin{table}[H]
  \centering
  \caption{Descriptive recall differences (LLM-only vs.\ LLM+RAG).}
  \label{tab:eval-recall-delta}
  \small
  \begin{tabular}{lccc}
    \toprule
    Model & Recall LLM (\%) & Recall RAG (\%) & $\Delta$ Recall (pp) \\
    \midrule
    \texttt{gemma3:1b} & 5.56 (3/54) & 0.00 (0/54) & -5.56 \\
    \texttt{gemma3:4b} & 42.59 (23/54) & 44.44 (24/54) & +1.85 \\
    \texttt{qwen3:4b} & 0.00 (0/54) & 0.00 (0/54) & +0.00 \\
    \texttt{qwen3:8b} & 33.33 (18/54) & 35.19 (19/54) & +1.86 \\
    \texttt{CodeLlama:latest} & 3.70 (2/54) & 0.00 (0/54) & -3.70 \\
    \bottomrule
  \end{tabular}
\end{table}

These differences are reported as run-level directional deltas. Because repeated evaluations reuse the same snippets and paired per-sample contingency outputs were not exported by the harness, these values should be interpreted as descriptive measurements for this run.

\subsection*{Limitations}

The dataset is intentionally small and synthetic; therefore, it may not capture the full diversity of real-world codebases (framework-specific patterns, multi-file flows, and complex validation logic). Future work should include evaluation on larger benchmarks and real repositories, as discussed in Chapter~\ref{chap:future-work}.

\subsection*{Negative Results and Boundary Conditions}

To avoid overstating system performance, the following negative outcomes are explicit boundary conditions of this thesis run:
\begin{itemize}
  \item \textbf{RAG is not uniformly beneficial:} for \texttt{gemma3:1b}, enabling RAG reduced recall from 5.56\% to 0.00\%.
  \item \textbf{Higher recall can come with high alert noise:} \texttt{gemma3:4b} flagged all secure evaluations in both modes (FPR 100\%).
  \item \textbf{Parse robustness can dominate measured quality:} \texttt{qwen3:4b} and \texttt{CodeLlama:latest} showed parse rates between 1.01\% and 5.05\%, with near-zero recall.
  \item \textbf{Run-level deltas are descriptive only:} mode-to-mode changes should be interpreted as measurements from this run, not inferential effects.
\end{itemize}

These results indicate that practical deployment requires explicit tuning of model, prompting, and scoring ontology rather than assuming a single universally best configuration.

\subsection*{Threats to Validity}

\begin{table}[H]
  \centering
  \caption{Threats-to-validity summary and mitigations.}
  \label{tab:eval-validity-summary}
  \footnotesize
  \renewcommand{\arraystretch}{1.15}
  \begin{tabularx}{\textwidth}{p{0.14\textwidth}XXX}
    \toprule
    Threat type & Main risk & Mitigation in this thesis & Residual risk \\
    \midrule
    Internal & Label-matching and issue-level FP counting can bias precision/FPR. & Fixed JSON schema, repeated runs, explicit reporting of scoring rules. & Partial label mismatches still distort measured precision/recall. \\
    Construct & Core metrics may miss practical developer value (localization, fix quality). & Added qualitative case studies and repair-quality criteria. & No fully automated metric for fix correctness/minimality yet. \\
    External & Curated synthetic snippets may not reflect real multi-file systems. & Included diverse CWE/OWASP-aligned classes and secure cases. & Generalization to production repositories remains uncertain. \\
    Measurement interpretation & Repeated measurements on the same snippets reduce independence of pooled counts. & Reported raw counts/percentages and explicit mode-to-mode recall deltas. & Paired per-sample mode comparison remains limited by current exported outputs. \\
    \bottomrule
  \end{tabularx}
\end{table}

\textbf{Internal validity.} The evaluation harness scores detections at the vulnerability-type level using substring matching between expected and predicted category names. This reduces brittleness to naming variation but can also over-credit partially correct labels (e.g., a broad ``Injection'' label matching multiple injection subclasses) or under-credit semantically correct but differently phrased labels. In addition, false positives are counted at issue level, so precision/FPR denominators are sensitive to how many issues a model emits per sample. A stricter label ontology plus CWE-level normalization would improve precision of the evaluation itself.

\textbf{Construct validity.} Precision/recall and parse success rate measure important properties for IDE integration, but they do not fully capture developer value. In practice, usefulness also depends on (i) localization quality (correct lines), (ii) explanation clarity, and (iii) the security adequacy and minimality of suggested fixes. These are addressed in part through qualitative case studies (Section~\ref{sec:eval-case-studies}) but are not yet fully quantified.

\textbf{External validity.} Synthetic snippets are easier than real codebases with frameworks, configuration files, and cross-file flows. Results on the curated datasets should therefore be interpreted as an estimate of core capability under controlled conditions rather than a definitive measure of real-world performance. The most likely failure modes in practice are missing context when only a small scope (e.g., a single function) is analyzed, and output-structure instability for certain model families.

\subsection*{Reproducibility Notes}

To support reproducibility, the benchmark suite and evaluation harness are kept alongside the prototype in the thesis workspace, and run-specific environment/configuration details are reported in Section~\ref{sec:eval-setup}. Repeating runs remains important for estimating variability due to runtime effects (warm-up, caching, and transient system load) even under low-temperature decoding.
