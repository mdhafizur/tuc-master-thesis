\section{Experimental Setup}
\label{sec:eval-setup}

All experiments are executed locally using the Code Guardian evaluation harness in \path{code-guardian/evaluation/evaluate-models.js}. The harness iterates over the curated dataset described in Section~\ref{sec:eval-dataset} and invokes Ollama with a strict JSON-only system prompt. Model decoding is configured for low randomness (\texttt{temperature=0.1}) to improve determinism and parsing stability.

\subsection*{Response Schema for Scoring}

For evaluation purposes, the harness requests a richer structured output than the minimal in-editor diagnostics flow. In particular, each finding includes a vulnerability \texttt{type} string and a coarse \texttt{severity} level in addition to location and an optional fix. This enables type-level scoring (precision/recall) and per-category analysis. In the extension UI, vulnerability categories may appear within the message text and are used for downstream aggregation (e.g., in the workspace dashboard); a future refinement is to surface \texttt{type} and \texttt{severity} as first-class fields in diagnostics.

\subsection*{Compared Configurations}

Two configurations are evaluated quantitatively:
\begin{itemize}
  \item \textbf{LLM-only:} JSON-only analyzer without retrieval context.
  \item \textbf{LLM+RAG:} retrieval-augmented prompting with static security snippets (\texttt{k=5}).
\end{itemize}

Where relevant, results can also be stratified by model size or model family to study the trade-off between accuracy and latency.

\subsection*{Models and Hardware}

The reported run evaluated five local Ollama models: \texttt{gemma3:1b}, \texttt{gemma3:4b}, \texttt{qwen3:4b}, \texttt{qwen3:8b}, and \texttt{CodeLlama:latest}. The execution environment was macOS (Darwin 25.3.0, arm64) on Apple M4 Max (16 CPU cores, 64\,GB RAM), Node.js v20.19.5, and Ollama 0.17.0.

\subsection*{Runtime Parameters}

The evaluation harness enforces a per-request timeout (\texttt{timeoutMs=30000}) and limits generation length (\texttt{num\_predict=1000}). A request delay of \texttt{500\,ms} is inserted between calls to reduce burstiness on developer hardware. Runs were executed sequentially with no warm-up and no retries.

\subsection*{Reproducibility Snapshot}

\begin{table}[H]
  \centering
  \caption{Run configuration used for reported results.}
  \label{tab:eval-reproducibility}
  \small
  \begin{tabular}{p{0.33\textwidth}p{0.6\textwidth}}
    \toprule
    Item & Value \\
    \midrule
    Dataset file & \texttt{vulnerability-test-cases.json} (33 cases: 18 vulnerable, 15 secure) \\
    Runs per sample & 3 \\
    Prompt modes & LLM-only, LLM+RAG \\
    RAG settings & static security snippets, \texttt{k=5} \\
    Generation settings & \texttt{temperature=0.1}, \texttt{num\_predict=1000} \\
    Request controls & \texttt{timeoutMs=30000}, \texttt{requestDelayMs=500} \\
    Execution mode & sequential, no warm-up, no retries \\
    Models requested & \texttt{gemma3:1b}, \texttt{gemma3:4b}, \texttt{qwen3:4b}, \texttt{qwen3:8b}, \texttt{CodeLlama:latest} \\
    Model resolution & All models resolved to the same tags as requested in this run \\
    Software & Ollama 0.17.0, Node.js v20.19.5 \\
    Hardware/OS & Apple M4 Max (16 CPU cores, 64 GB RAM), macOS Darwin 25.3.0 (arm64) \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Artifact Provenance}

To document run provenance, Table~\ref{tab:eval-artifact-manifest} records the metadata fingerprint for the reported run.

\begin{table}[H]
  \centering
  \caption{Artifact provenance manifest for this thesis run.}
  \label{tab:eval-artifact-manifest}
  \footnotesize
  \begin{tabularx}{\textwidth}{p{0.34\textwidth}X}
    \toprule
    Field & Value \\
    \midrule
    Report repository commit & not captured by the harness in this run \\
    Run window (UTC) & 2026-02-25 18:26:56 to 2026-02-25 20:00:54 \\
    Total runtime & 5\,638\,211 ms \\
    Execution policy & sequential order, no retries, no warm-up \\
    Dataset path & \path{code-guardian/evaluation/datasets/vulnerability-test-cases.json} \\
    Invocation command & \texttt{node evaluation/evaluate-models.js --ablation --runs=3 --rag-k=5 --temperature=0.1 --num-predict=1000 --timeout-ms=30000 --delay-ms=500} \\
    Model fingerprint (\texttt{gemma3:1b}) & digest prefix \texttt{8648f39daa8f} \\
    Model fingerprint (\texttt{gemma3:4b}) & digest prefix \texttt{a2af6cc3eb7f} \\
    Model fingerprint (\texttt{qwen3:4b}) & digest prefix \texttt{359d7dd4bcda} \\
    Model fingerprint (\texttt{qwen3:8b}) & digest prefix \texttt{500a1f067a9f} \\
    Model fingerprint (\texttt{CodeLlama:latest}) & digest prefix \texttt{8fdf8f752f6e} \\
    \bottomrule
  \end{tabularx}
\end{table}

For archival reproducibility, the recommended artifact bundle includes: (i) raw JSON output from the harness, (ii) the exact run configuration object, (iii) model digests, and (iv) generated summary tables. These artifacts should be stored as supplementary material together with the thesis PDF.

\subsection*{Run-to-Run Variability (Descriptive)}

Each sample is evaluated three times in each configuration. Reported values are descriptive percentages and counts over pooled evaluations (e.g., recall over vulnerable evaluations, parse success over total requests). Because repeated runs reuse the same snippets and paired per-sample mode outputs were not exported by the harness, mode-to-mode differences are interpreted descriptively for this run only.

\paragraph{Response parsing.}
The harness strips Markdown code fences if present and then attempts to parse the remaining content as JSON. Responses that fail to parse are counted toward the parse failure rate and are scored as producing no issues, which impacts recall on vulnerable samples.

\subsection*{Running the Evaluation}

The evaluation can be reproduced by running:
\begin{lstlisting}[language=Java, caption={Running the Code Guardian evaluation harness}, label={lst:eval-run}]
cd code-guardian
node evaluation/evaluate-models.js --ablation --runs=3 --rag-k=5 --temperature=0.1 \
  --num-predict=1000 --timeout-ms=30000 --delay-ms=500
\end{lstlisting}

The script produces per-model metrics and can be extended to emit JSON/Markdown reports under \path{code-guardian/evaluation/logs/} for inclusion in the thesis appendix. For strict comparability with the reported tables, the evaluated model list should match the run configuration (\texttt{gemma3:1b}, \texttt{gemma3:4b}, \texttt{qwen3:4b}, \texttt{qwen3:8b}, and \texttt{CodeLlama:latest}).

The reported results in this thesis were produced with \texttt{runsPerSample=3} for both prompt modes.
