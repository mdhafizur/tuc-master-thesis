\section{Datasets}
\label{sec:eval-dataset}

The evaluation dataset is a project-authored curated set of JavaScript and TypeScript security cases maintained alongside the Code Guardian prototype in \path{code-guardian/evaluation/datasets/}. Cases are aligned to CWE/OWASP concepts. Each test case consists of (i) a short code snippet, (ii) a list of expected vulnerabilities with CWE identifiers and severities, and (iii) an expected remediation description. Two datasets are provided:
\begin{itemize}
  \item \textbf{Core dataset:} \texttt{vulnerability-test-cases.json} contains representative examples for common vulnerability classes (e.g., SQL injection, XSS, command injection, path traversal) as well as a small number of secure examples to measure false positives.
  \item \textbf{Advanced dataset:} \texttt{advanced-test-cases.json} contains additional vulnerability types and more nuanced patterns (e.g., insecure CORS configuration, timing attacks, mass assignment).
\end{itemize}

In the evaluated prototype version used for this thesis run, the scored dataset (\texttt{vulnerability-test-cases.json}) contains \textbf{33} test cases (\textbf{18} vulnerable and \textbf{15} secure). Expected findings are annotated with CWE identifiers to support aggregation by vulnerability class.

\paragraph{Which dataset is scored in this chapter.}
The repository-contained evaluation harness used in this thesis (\path{code-guardian/evaluation/evaluate-models.js}) was run in ablation mode with \texttt{--ablation --runs=3}. Unless stated otherwise, quantitative results in Chapter~\ref{chap:evaluation} refer to this 33-case dataset.

Vulnerability classes are aligned with the Common Weakness Enumeration taxonomy \cite{mitreCWE}, and severities are recorded as coarse labels to support prioritization and reporting. Where applicable, severity can be related back to standardized scoring systems such as CVSS and public vulnerability databases such as CVE/NVD \cite{firstCvss31,mitreCVE,nistNVD}.

This dataset targets requirement R2 (context-aware vulnerability reasoning) by including vulnerability instances that cannot be resolved purely by superficial keyword matching (e.g., sink usage that requires reasoning about tainted input). It also supports R1 by enabling reproducible comparisons across repeated runs and across different local models.

\subsection*{Dataset Schema}

Each record follows a uniform schema:
\begin{itemize}
  \item \texttt{id}: unique identifier
  \item \texttt{name}: descriptive test name
  \item \texttt{code}: code snippet under test
  \item \texttt{expectedVulnerabilities}: list of expected findings (type, CWE, severity, description)
  \item \texttt{expectedFix}: short remediation guidance (optional)
\end{itemize}

The dataset is intentionally small and human-auditable to facilitate iteration on prompts, retrieval, and output validation. Limitations of synthetic snippets and representativeness are discussed in Section~\ref{sec:eval-summary}.

\paragraph{Toward larger benchmarks.}
While this thesis focuses on a curated, auditable suite to support rapid iteration, standard benchmark suites such as the OWASP Benchmark and NIST Juliet can be used to broaden coverage and stress-test generalization in future work \cite{owaspBenchmark,nistJuliet}. In addition, secure coding checklists and verification frameworks such as OWASP ASVS can guide the selection of security requirements and test categories when scaling the evaluation \cite{owaspASVS}.
