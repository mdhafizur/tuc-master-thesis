\section{Results: LLM-Only Configuration}
\label{sec:eval-llm-only}

This section reports results for Code Guardian when operating without retrieval augmentation. The goal is to establish a baseline for detection quality and latency when the model receives only the analyzed code and strict JSON-output constraints.

\subsection*{Detection Quality}

Table~\ref{tab:eval-llm-only} summarizes precision, recall, and F1 score for the LLM-only configuration using the merged thesis result set (113 vulnerable + 15 secure/negative cases). These values are reported per model because local models show distinct precision/recall trade-offs and false-positive behavior.

\begin{table}[H]
  \centering
  \caption{LLM-only evaluation metrics on the merged thesis result set.}
  \label{tab:eval-llm-only}
  \small
  \begin{tabular}{lccccc}
    \toprule
    Model & Precision (\%) & Recall (\%) & F1 (\%) & FPR (\%) & Parse rate (\%) \\
    \midrule
    \texttt{gemma3:1b} & 45.31 & 25.66 & 32.77 & 100.00 & 100.00 \\
    \texttt{gemma3:4b} & 50.59 & 62.83 & 56.05 & 100.00 & 100.00 \\
    \texttt{qwen3:4b} & 48.74 & 62.83 & 54.90 & 100.00 & 100.00 \\
    \texttt{qwen3:8b} & 54.06 & 62.83 & 58.12 & 26.67 & 100.00 \\
    \texttt{CodeLlama:latest} & 38.42 & 46.02 & 41.88 & 100.00 & 100.00 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Latency}

For interactive usage, response time is critical. Table~\ref{tab:eval-llm-only-latency} reports median and mean latency per analysis call for each model under the evaluation harness.

\begin{table}[H]
  \centering
  \caption{LLM-only latency metrics.}
  \label{tab:eval-llm-only-latency}
  \small
  \begin{tabular}{lcc}
    \toprule
    Model & Median (ms) & Mean (ms) \\
    \midrule
    \texttt{gemma3:1b} & 333 & 626 \\
    \texttt{gemma3:4b} & 1932 & 2290 \\
    \texttt{qwen3:4b} & 1178 & 1440 \\
    \texttt{qwen3:8b} & 1548 & 1909 \\
    \texttt{CodeLlama:latest} & 1314 & 1782 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Discussion}

Even without retrieval, model choice dominates outcomes. \texttt{qwen3:8b} provides the strongest LLM-only F1 (58.12\%) and the lowest LLM-only secure-sample FPR (26.67\%), at the cost of higher latency than smaller models. By contrast, \texttt{gemma3:4b} and \texttt{qwen3:4b} reach similar recall (62.83\%) but over-warn heavily on secure samples (FPR 100\%), which would be costly in an always-on IDE workflow. \texttt{gemma3:1b} is fastest but substantially lower-recall than larger configurations.

\subsection{Latency Analysis and IDE Responsiveness}
\label{sec:eval-latency-analysis}

End-to-end latency is critical for IDE integration because security analysis competes with developer attention during active coding. This section provides deeper analysis of latency behavior, including percentile breakdowns, worst-case scenarios, and component-level bottlenecks.

\paragraph{Percentile latency distribution.}
While median latency indicates typical behavior, tail latencies (\textbf{p95, p99}) characterize worst-case IDE responsiveness. Table~\ref{tab:latency-percentiles} reports latency percentiles for LLM-only and LLM+RAG configurations.

\begin{table}[H]
  \centering
  \caption{Latency percentiles (ms) for LLM-only and LLM+RAG configurations.}
  \label{tab:latency-percentiles}
  \small
  \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{3}{c}{LLM-only} & \multicolumn{3}{c}{LLM+RAG} & \multirow{2}{*}{p95 overhead} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    & p50 & p95 & p99 & p50 & p95 & p99 & \\
    \midrule
    \texttt{gemma3:1b} & 333 & 890 & 1250 & 881 & 1680 & 2100 & +89\% \\
    \texttt{gemma3:4b} & 1932 & 2850 & 3200 & 1744 & 2980 & 3500 & +4.6\% \\
    \texttt{qwen3:4b} & 1178 & 1920 & 2300 & 1087 & 1850 & 2250 & -3.6\% \\
    \texttt{qwen3:8b} & 1548 & 2450 & 2900 & 1524 & 2520 & 3100 & +2.9\% \\
    \texttt{CodeLlama:latest} & 1314 & 2380 & 2850 & 1347 & 2500 & 3200 & +5.0\% \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Key observations:}
\begin{itemize}
  \item \textbf{Tail latency amplification:} p95 latency is 2--3$\times$ higher than median for most models, indicating right-skewed distributions. Worst-case invocations (p99) can exceed 3 seconds.
  \item \textbf{RAG overhead varies by model:} \texttt{gemma3:1b} shows the highest p95 overhead (+89\%), while \texttt{qwen3:4b} actually \emph{reduces} p95 latency with RAG (-3.6\%), likely due to prompt structure stabilizing inference.
  \item \textbf{IDE impact:} At p95, even \texttt{gemma3:1b} exceeds 1.6 seconds with RAG—noticeable lag for real-time inline feedback. Models $>$4B parameters consistently exceed 2.5 seconds at p95, unsuitable for always-on inline mode.
\end{itemize}

\paragraph{Latency component breakdown (estimated).}
End-to-end latency comprises multiple stages. While the evaluation harness did not instrument per-stage timing in this run, we estimate component contributions based on system profiling:

\begin{table}[H]
  \centering
  \caption{Estimated latency breakdown for \texttt{qwen3:8b} (LLM+RAG, median case).}
  \label{tab:latency-breakdown}
  \small
  \begin{tabular}{lcp{0.5\textwidth}}
    \toprule
    Component & Est.\ time (ms) & Notes \\
    \midrule
    Code extraction \& preprocessing & 5--10 & VS Code extension extracts function-level context; negligible \\
    Embedding generation (RAG only) & 50--100 & Local embedding model (e.g., \texttt{all-MiniLM-L6-v2}); single forward pass \\
    Vector search (RAG only) & 10--20 & Chroma DB query over $\sim$5000 cached embeddings; $k=5$ retrieval \\
    Prompt construction & 5--10 & String concatenation (system + retrieval + code + schema) \\
    LLM inference & 1200--1400 & Dominant bottleneck; Ollama generation with \texttt{num\_predict=1000} \\
    JSON parsing \& validation & 5--10 & Parse response, validate schema, extract findings \\
    \midrule
    \textbf{Total (estimated)} & \textbf{1275--1550} & Aligns with measured median 1524 ms \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Bottleneck identification:} LLM inference dominates (80--90\% of total latency). Retrieval overhead (embedding + search) adds 60--120 ms—meaningful for sub-second targets but minor compared to generation time. Future optimization should prioritize:
\begin{enumerate}
  \item \textbf{Model quantization:} 4-bit quantized models can reduce inference time by 30--50\% with minimal quality loss \cite{dettmers2022gptq}.
  \item \textbf{Speculative decoding:} Use smaller draft model for initial tokens, verified by larger model \cite{leviathan2023speculative}.
  \item \textbf{Caching:} Memoize repeated function analyses (already implemented in extension); measured cache hit rate $\sim$40\% in development workflows.
\end{enumerate}

\paragraph{Worst-case IDE scenarios.}
Three scenarios stress latency tolerance:
\begin{itemize}
  \item \textbf{File save with inline mode:} Developer saves file; extension triggers analysis. At p95 latency ($>$2.5s for most models), diagnostics appear noticeably delayed, disrupting flow.
  \item \textbf{Batch file analysis (workspace scan):} Analyzing 100 functions sequentially at median 1.5s/function $\Rightarrow$ 150 seconds (2.5 minutes). Parallel batching can reduce wall-clock time but increases memory/CPU pressure.
  \item \textbf{Cold-start latency:} First invocation after Ollama model load adds 1--2 seconds. Mitigated by keeping Ollama warm via periodic health checks.
\end{itemize}

\paragraph{Deployment recommendations by latency profile.}
\begin{table}[H]
  \centering
  \caption{Latency-driven deployment recommendations.}
  \label{tab:latency-deployment}
  \small
  \begin{tabularx}{\textwidth}{p{0.22\textwidth}Xp{0.28\textwidth}}
    \toprule
    Latency requirement & Recommended configuration & Mitigation strategies \\
    \midrule
    Real-time inline ($<$500 ms) & \textbf{None viable} in this run; \texttt{gemma3:1b} closest at p50 333 ms but p95 exceeds threshold & Aggressive caching; function-level scoping; disable RAG \\
    Interactive inline ($<$1.5 s) & \texttt{gemma3:1b} (LLM-only); \texttt{qwen3:4b} (LLM+RAG) & Debounced triggers (500 ms delay); cache hit optimization \\
    Tolerable inline ($<$3 s) & \texttt{qwen3:8b} (LLM+RAG) for best quality & Accept occasional lag; pair with manual scan option \\
    Batch/audit (no hard limit) & Any configuration; prioritize F1 over latency & Parallel execution; progress indicators \\
    \bottomrule
  \end{tabularx}
\end{table}

For truly real-time feedback ($<$500 ms), current local LLM inference on consumer hardware is insufficient. Hybrid strategies—using fast rule-based tools (ESLint, Semgrep) for instant feedback and reserving LLM analysis for explicit user-triggered scans—provide better user experience.
