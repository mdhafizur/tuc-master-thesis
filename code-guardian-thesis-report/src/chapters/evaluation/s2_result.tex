\section{Results: LLM+RAG Configuration}
\label{sec:eval-rag}

This section reports Code Guardian with retrieval augmentation enabled. In this configuration, the prompt is enriched with locally retrieved security knowledge (CWE/OWASP/CVE-derived summaries and mitigation guidance) to ground the modelâ€™s output.

\subsection*{Detection Quality}

Table~\ref{tab:eval-rag} summarizes detection metrics for the RAG-enhanced configuration. Comparing Table~\ref{tab:eval-rag} against Table~\ref{tab:eval-llm-only} isolates the empirical impact of retrieval augmentation on precision/recall trade-offs.

\begin{table}[H]
  \centering
  \caption{LLM+RAG evaluation metrics on the curated dataset.}
  \label{tab:eval-rag}
  \small
  \begin{tabular}{lccccc}
    \toprule
    Model & Precision (\%) & Recall (\%) & F1 (\%) & FPR (\%) & Parse rate (\%) \\
    \midrule
    \texttt{gemma3:1b} & 0.00 & 0.00 & 0.00 & 0.00 & 100.00 \\
    \texttt{gemma3:4b} & 24.24 & 44.44 & 31.37 & 100.00 & 100.00 \\
    \texttt{qwen3:4b} & 0.00 & 0.00 & 0.00 & 0.00 & 4.04 \\
    \texttt{qwen3:8b} & 54.29 & 35.19 & 42.70 & 27.12 & 84.85 \\
    \texttt{CodeLlama:latest} & 0.00 & 0.00 & 0.00 & 10.00 & 1.01 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Latency Overhead}

Retrieval introduces overhead from embedding, vector search, and prompt expansion. Table~\ref{tab:eval-rag-latency} reports the latency impact of enabling RAG under the same evaluation harness.

\begin{table}[H]
  \centering
  \caption{LLM+RAG latency metrics.}
  \label{tab:eval-rag-latency}
  \small
  \begin{tabular}{lcc}
    \toprule
    Model & Median (ms) & Mean (ms) \\
    \midrule
    \texttt{gemma3:1b} & 180 & 180 \\
    \texttt{gemma3:4b} & 1335 & 1334 \\
    \texttt{qwen3:4b} & 9852 & 9959 \\
    \texttt{qwen3:8b} & 7809 & 9012 \\
    \texttt{CodeLlama:latest} & 5478 & 5768 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Discussion}

RAG effects are model-dependent in this run. For \texttt{qwen3:8b}, RAG improved precision and F1, reduced FPR, and reduced median latency compared to LLM-only. \texttt{gemma3:4b} showed a small F1 gain but retained an FPR of 100\%. For \texttt{gemma3:1b}, RAG reduced recall to zero. \texttt{qwen3:4b} and \texttt{CodeLlama:latest} remained dominated by parse failures in both prompt modes.
