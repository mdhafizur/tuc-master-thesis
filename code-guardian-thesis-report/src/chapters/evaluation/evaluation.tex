This chapter evaluates Code Guardian with respect to the requirements defined in Chapter~\ref{chap:analysis}. The evaluation focuses on three aspects: (i) detection quality (precision/recall trade-offs and false positives), (ii) robustness of structured output (JSON parse success), and (iii) responsiveness (latency under IDE usage patterns). In addition, we report qualitative observations on repair suggestions and developer-facing usability.

The evaluation is designed as a pragmatic engineering assessment rather than a pure benchmark competition. Accordingly, the chapter combines quantitative metrics, baseline comparison, and qualitative case analysis to answer three practical questions: (i) when the assistant is useful, (ii) where it fails, and (iii) which deployment profiles are defensible under local privacy constraints. Throughout this chapter, results are interpreted descriptively for the documented run environment and configuration.

\input{src/chapters/evaluation/dataset_details}
\input{src/chapters/evaluation/evaluation_metrics}
\input{src/chapters/evaluation/experimental_setup}
\input{src/chapters/evaluation/s1_result}
\input{src/chapters/evaluation/s2_result}
\input{src/chapters/evaluation/s3_result}
\input{src/chapters/evaluation/s4_result}
\input{src/chapters/evaluation/summary}
