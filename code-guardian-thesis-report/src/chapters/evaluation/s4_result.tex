\section{Qualitative Case Studies and Repair Suggestions}
\label{sec:eval-case-studies}

Quantitative metrics summarize overall detection behavior, but IDE usefulness also depends on explanation clarity and repair quality. This section reports qualitative observations from representative cases in the recorded ablation run.

\subsection*{Representative Cases}

\textbf{Case A (True Positive with direct remediation): SQL injection.}
Sample ID: \texttt{sql-injection-1}.
In the best-performing configuration (\texttt{qwen3:8b} with RAG), the model flagged direct string concatenation in the query and suggested parameterized queries. The suggested remediation aligned with the expected fix and was minimal.

\textbf{Case B (False Positive on secure sample): safe process invocation.}
Sample ID: secure snippet with \texttt{execFile} array arguments.
In \texttt{qwen3:8b} (LLM+RAG), this secure sample was still flagged, with a broad path-sanitization recommendation. This illustrates residual over-warning even in the strongest configuration.

\textbf{Case C (False Negative): NoSQL injection.}
Sample ID: NoSQL injection representative case.
In \texttt{qwen3:8b} (LLM+RAG), this vulnerable sample was missed in a representative run and no remediation suggestion was produced.

\textbf{Case D (True Positive): cross-site scripting via \texttt{innerHTML}.}
Sample ID: XSS representative case.
In \texttt{qwen3:8b} (LLM+RAG), the model correctly flagged unsafe HTML sink usage and proposed replacing \texttt{innerHTML} with safer rendering behavior.

\subsection*{Repair Suggestion Quality (R4)}

Repair suggestions are evaluated qualitatively against three criteria:
\begin{itemize}
  \item \textbf{Security adequacy:} does the fix mitigate the vulnerability class (e.g., parameterization for SQL injection)?
  \item \textbf{Minimality:} does it avoid unnecessary refactoring?
  \item \textbf{Applicability:} does it fit the code context and available APIs?
\end{itemize}

Because Code Guardian operates inside the IDE, repair suggestions are intentionally presented as \emph{optional} quick fixes, enabling developer review and preventing silent modifications.

\subsection*{Observed Failure Modes}

Observed failure modes in the run include:
\begin{itemize}
  \item \textbf{Over-warning:} many secure patterns are still flagged as vulnerable in multiple configurations.
  \item \textbf{Model-dependent recall shifts under RAG:} some models improve (notably \texttt{qwen3} variants), while others degrade.
  \item \textbf{Conservative under-warning:} low-latency configurations can miss many vulnerable samples.
  \item \textbf{High-latency audit modes:} strongest F1 configurations may still be too slow for inline workflows.
\end{itemize}

\subsection*{Error Taxonomy from Run Logs}

\begin{table}[H]
  \centering
  \caption{Observed error taxonomy in the ablation run.}
  \label{tab:eval-error-taxonomy}
  \footnotesize
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{p{0.28\textwidth}p{0.30\textwidth}p{0.30\textwidth}}
    \toprule
    Error pattern & Representative evidence & Quantitative impact in this run \\
    \midrule
    Persistent secure-sample over-warning & \texttt{gemma3:4b}, \texttt{qwen3:4b}, \texttt{CodeLlama:latest} (both modes) & FPR remained 100\% in merged evaluation (all secure samples were flagged in these modes). \\
    Model-dependent RAG effect & \texttt{qwen3} vs.\ \texttt{gemma3:4b}/\texttt{CodeLlama} & RAG improved \texttt{qwen3:8b} (F1 58.12\% $\rightarrow$ 63.76\%) and \texttt{qwen3:4b} (54.90\% $\rightarrow$ 58.67\%), but reduced \texttt{gemma3:4b} and \texttt{CodeLlama:latest}. \\
    Conservative under-detection & \texttt{gemma3:1b} (LLM-only) & Recall remained comparatively low at 25.66\% (87/339) despite fast latency. \\
    High-latency high-F1 mode & \texttt{qwen3:8b} (LLM+RAG) & Best F1 (63.76\%) and best LLM FPR (26.67\%) came with higher latency (median 1524\,ms; mean 1827\,ms). \\
    \bottomrule
  \end{tabular}
\end{table}

These observations motivate the future work directions summarized in Chapter~\ref{chap:future-work}.
