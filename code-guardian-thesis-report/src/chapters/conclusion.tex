This chapter summarizes the contributions of this thesis, highlights key findings, discusses limitations, and outlines implications for practice. The goal of the thesis was to design and implement a privacy-preserving vulnerability detection and repair assistant for Visual Studio Code that leverages locally deployed LLMs and retrieval-augmented grounding without transmitting source code to external services.

\section{Summary of Contributions}

\textbf{Code Guardian: an IDE-integrated, local security assistant.}
This thesis delivers \textbf{Code Guardian}, a VS Code extension that performs on-device vulnerability analysis for JavaScript and TypeScript projects. Findings are presented using IDE-native diagnostics, and optional quick fixes provide repair suggestions while keeping developers in full control of code changes.

\textbf{Privacy-preserving LLM inference with optional RAG.}
All code analysis runs locally via Ollama. To improve grounding, the system optionally augments prompts with locally retrieved security knowledge (CWE/OWASP/CVE guidance) using a local vector index and local embeddings. This architecture supports explainability and consistency while preserving the no-exfiltration requirement.

\textbf{Practical performance mechanisms.}
To remain usable during development, Code Guardian combines debounced triggers, function-level scoping for real-time use, and caching of repeated analyses. These mechanisms reduce unnecessary inference calls and support responsive IDE feedback.

\textbf{Reproducible evaluation harness.}
The prototype includes a curated benchmark of security test cases and a local evaluation script for comparing models and configurations using standard detection metrics, parse robustness, and latency.

\section{Answers to Research Questions}

\textbf{RQ1 (Feasibility).}
The thesis supports feasibility with caveats. Code Guardian demonstrates that useful vulnerability analysis and repair suggestions can be generated inside VS Code using fully local inference, satisfying the no-code-exfiltration objective. However, practical usefulness depends on selecting a model profile that matches the deployment context (interactive editing vs.\ audit workflows).

\textbf{RQ2 (Grounding).}
Retrieval augmentation is beneficial only under model-dependent conditions. In this run, RAG improved \texttt{qwen3:8b} (F1 58.12\% $\rightarrow$ 63.76\%) and \texttt{qwen3:4b} (F1 54.90\% $\rightarrow$ 58.67\%), but reduced \texttt{gemma3:4b} (56.05\% $\rightarrow$ 49.68\%) and \texttt{CodeLlama:latest} (41.88\% $\rightarrow$ 31.45\%). Grounding should therefore be treated as a tunable strategy rather than a universally positive default.

\textbf{RQ3 (Practicality).}
IDE practicality is achievable when inference is constrained by function-level scoping, debounced triggers, and caching. These mechanisms keep local analysis responsive for small models, while larger models remain better suited to on-demand scans where higher latency is acceptable.

\section{Deployment Implications}

Table~\ref{tab:conclusion-deployment-profiles} summarizes pragmatic deployment choices based on the measured trade-offs.

\begin{table}[H]
  \centering
  \caption{Recommended deployment profiles from thesis results.}
  \label{tab:conclusion-deployment-profiles}
  \small
  \begin{tabularx}{\textwidth}{p{0.19\textwidth}p{0.22\textwidth}XX}
    \toprule
    Use case & Preferred configuration & Main advantage & Main risk \\
    \midrule
    Real-time editor feedback & \texttt{gemma3:1b} (LLM-only) & Lower latency profile (about 333 ms median) & Lower recall than higher-capacity models (25.66\%) and severe secure-sample over-warning (FPR 100\%) \\
    Moderate-latency audit pass & \texttt{qwen3:4b} (LLM+RAG) & Good recall (64.90\%) with about 1.1 s median latency & Severe alert noise on secure samples (FPR 100\%) \\
    Higher-quality deep audit & \texttt{qwen3:8b} (LLM+RAG) & Best F1 (63.76\%) and best LLM FPR (26.67\%) in this run & Higher latency (median 1524 ms) and still non-trivial false-positive behavior \\
    High-noise model profiles & \texttt{gemma3:4b} / \texttt{qwen3:4b} / \texttt{CodeLlama:latest} & Strong vulnerable recall in some modes & Persistent secure-sample over-warning (FPR 100\%) \\
    \bottomrule
  \end{tabularx}
\end{table}

\section{Limitations}

\textbf{Scope limits and contextual depth.}
While the system can flag common vulnerability patterns, deep semantic reasoning across files (e.g., source-to-sink flows spanning modules) is limited by the analysis scope and the absence of full static data-flow analysis.

\textbf{Evaluation representativeness.}
The curated dataset is intentionally small and human-auditable, but it does not fully reflect the diversity and ambiguity of real-world codebases. Results should therefore be interpreted as indicative rather than definitive.

\textbf{Repair correctness.}
Repair suggestions are model-generated and may affect behavior beyond security hardening. The system mitigates this by requiring explicit user review, but comprehensive functional validation remains outside the scope of the extension.

\section{Responsible Use}

This thesis treats Code Guardian as a decision-support system, not an autonomous security verifier. Both false negatives and false positives were observed, and some findings used semantically plausible but ontology-mismatched labels. For this reason, findings and repair suggestions should remain reviewable artifacts under developer control, with conventional testing and security review retained as mandatory safeguards before release.

\section{Conclusion}

Privacy-preserving secure coding assistance is feasible within the IDE when local LLM inference is combined with careful prompt structuring, optional retrieval grounding, and developer-controlled remediation workflows. The evaluation shows a clear quality-latency-robustness trade-off: \texttt{gemma3:1b} remained relatively fast (about 0.3--0.9 s median) but lower recall than larger models, \texttt{gemma3:4b} and \texttt{qwen3:4b} reached higher recall but over-warned on secure code, and \texttt{qwen3:8b} achieved the best F1 with higher per-call latency.

Compared with the requested SAST baselines, LLM modes in this run provided substantially higher vulnerable-case recall (up to 64.90\% vs.\ 9.73\% for Semgrep/CodeQL) but generally weaker false-positive control on secure samples (most LLM modes at 100\% FPR, \texttt{qwen3:8b} at 26.67\%, versus 6.67\% for Semgrep).

The results also show that retrieval augmentation is not universally beneficial; its effect is model-dependent. In this study, RAG improved both \texttt{qwen3} variants but degraded \texttt{gemma3:4b} and \texttt{CodeLlama:latest}. This indicates that RAG integration must be calibrated per model and prompt format, not assumed to improve security detection by default.

Overall, Code Guardian demonstrates that locally deployed LLMs can provide useful vulnerability detection and repair suggestions without transmitting source code off-device, but practical deployment requires explicit configuration choices for model class, latency budget, and acceptable false-positive behavior.
