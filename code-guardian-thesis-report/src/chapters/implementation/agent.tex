\section{Core Component Implementation}
\label{sec:impl-components}

This section details the concrete implementation of Code Guardian’s main components. The prototype is implemented as a single VS Code extension located in \texttt{code-guardian/}. Its primary tasks are (i) extracting relevant code context inside the IDE, (ii) invoking local LLM inference, (iii) optionally grounding the prompt with retrieved security knowledge (RAG), and (iv) presenting findings and repairs in an IDE-native way.

\subsection{Extension Entry Point and Event Wiring}
\label{subsec:impl-extension}

The extension entry point registers event listeners and commands. Real-time analysis is triggered on document changes and is debounced (default: 800\,ms) to avoid excessive inference calls during typing. On-demand commands support analyzing a selection, a full file, or scanning the workspace. Findings are reported through the VS Code diagnostics API so they appear inline and in the Problems panel.

\subsection{Context Extraction and Scoping}
\label{subsec:impl-context}

Code Guardian extracts the smallest useful unit of code for interactive analysis: the enclosing function at the cursor position. This design reduces prompt size and improves responsiveness without requiring full-program analysis. Function extraction is implemented as a lightweight syntactic pass (\texttt{code-guardian/src/functionExtractor.ts}) and returns both the extracted snippet and its start-line offset within the original document. When a snippet is analyzed, the offset is later used to map model-reported line numbers back to the full file, so that diagnostics are placed correctly in the editor.

For explicit commands, the scope expands to (i) a selected region (or current line if no selection exists) and (ii) the full file. These scopes prioritize completeness and are typically used for deeper inspection or before committing changes.

\subsection{LLM Analyzer (Local JSON-Only Output)}
\label{subsec:impl-analyzer}

The analyzer performs local inference through Ollama and requires the model to return \emph{only} a JSON array of findings. The output schema is intentionally minimal to keep parsing robust and IDE rendering straightforward.

\begin{lstlisting}[language=TypeScript, caption={Security issue output schema used by Code Guardian}, label={lst:cg-issue-schema}]
interface SecurityIssue {
  message: string;
  startLine: number;   // 1-based
  endLine: number;     // 1-based
  suggestedFix?: string;
}
\end{lstlisting}

To increase reliability, the implementation includes:
\begin{itemize}
  \item \textbf{Schema-constrained prompting:} the system prompt instructs strict JSON-only output.
  \item \textbf{Defensive parsing:} Markdown fences and stray quotes are removed, and the first JSON array substring is extracted when needed.
  \item \textbf{Retry logic:} transient errors (timeouts, temporary unavailability) are retried with exponential backoff.
\end{itemize}

\paragraph{Failure handling and safe defaults.}
In a developer tool, a false crash is often worse than a missed warning because it interrupts the workflow. Therefore, non-recoverable failures (e.g., repeated timeouts, model-not-found) are handled by showing a user-facing message and returning an empty issue list. This keeps the editor responsive while preserving explicit control over model configuration (e.g., prompting the user to pull a missing model).

\subsection{Diagnostics Mapping and Localization}
\label{subsec:impl-localization}

The diagnostic adapter converts the model’s 1-based line numbering into VS Code’s 0-based ranges and clamps indices to valid document bounds (\texttt{code-guardian/src/diagnostic.ts}). When a function snippet is analyzed, the previously computed start-line offset is added to each finding’s range. If a suggested fix is included, it is attached to the diagnostic as related information and surfaced as a quick-fix action.

\subsection{RAG Manager (Local Retrieval)}
\label{subsec:impl-rag}

When enabled, the RAG manager maintains a local security knowledge base and a persistent vector index. Knowledge items are embedded using a local embedding model accessed through Ollama (\texttt{nomic-embed-text}) and stored in an HNSW vector store. For each analysis request, the manager retrieves the top-$k$ relevant snippets and augments the prompt with (i) short vulnerability definitions and (ii) mitigation guidance. This grounding supports consistency and explainability (R1--R3).

\paragraph{Indexing and chunking.}
Knowledge entries are chunked using a recursive text splitter to balance semantic coherence and retrieval recall. The index is persisted under the extension’s storage path so it can be reused across sessions without rebuilding. RAG initialization is performed lazily at runtime to avoid slowing down extension activation when retrieval is not used.

\subsection{Vulnerability Knowledge Updates}
\label{subsec:impl-knowledge}

To keep retrieved content current, the vulnerability data manager periodically refreshes public security sources (e.g., OWASP Top~10 entries, a curated set of CWE patterns, and a configurable number of recent CVEs). Retrieved metadata is cached on disk and only public vulnerability information is fetched. No user source code or extracted code context is transmitted off-device, preserving the privacy goal (R5).

\paragraph{Offline fallback.}
Because network access may be restricted in sensitive environments, the system includes a minimal baseline knowledge bundle that is used when updates fail. This ensures that RAG-enabled prompting remains functional offline, even though coverage is reduced relative to a refreshed knowledge base.

\subsection{Diagnostics and Quick Fixes}
\label{subsec:impl-diagnostics}

Findings are mapped to VS Code diagnostics with appropriate text ranges. If the model provides a \texttt{suggestedFix}, a quick-fix code action is offered. Fixes are never applied automatically: the developer must confirm application, which maintains human control and reduces the risk of unintended behavioral changes (R4).

\subsection{Caching and Responsiveness Mechanisms}
\label{subsec:impl-cache}

To avoid repeated inference on unchanged snippets, Code Guardian caches analysis results keyed by the code snippet, active model, and prompt mode (RAG enabled/disabled) (\texttt{code-guardian/src/analysisCache.ts}). In addition to caching, real-time analysis is debounced to reduce request volume during rapid edits. Together, these mechanisms reduce redundant local LLM calls and make continuous feedback feasible on developer hardware (R6), while also improving run-to-run consistency by limiting stochastic re-analysis.

\subsection{Workspace Scanner and Dashboard}
\label{subsec:impl-workspace}

For project-level visibility, Code Guardian includes a workspace scanner that analyzes JavaScript and TypeScript files in batch and aggregates results into a dashboard WebView. The dashboard summarizes severity distribution and surfaces the most vulnerable files, supporting risk-based prioritization and iterative hardening.

The scanner enumerates files by extension, excludes dependency folders (e.g., \texttt{node\_modules}), and skips very large files to bound runtime. Because the JSON-only analyzer does not mandate a severity field, the scanner applies a conservative keyword-based severity heuristic to support coarse prioritization in the dashboard; this is treated as a presentation aid rather than a ground-truth classifier.

\subsection{Privacy Boundary and Threat Model Considerations}
\label{subsec:impl-privacy-boundary}

Code Guardian’s privacy boundary is defined at the point of inference: analyzed code and any extracted context are sent only to the local Ollama server. The system does not transmit source code to external services. When knowledge updates are enabled, only public vulnerability metadata is fetched; this data is cached locally and then used to ground prompts.

From a threat-model perspective, the main risks are \emph{prompt injection} (attacker-controlled comments or strings that attempt to override the analysis instruction) and \emph{retrieval poisoning} (malicious or misleading knowledge entries). The current prototype mitigates these risks primarily through strict output contracts and by keeping retrieval sources scoped to curated security data; Chapter~\ref{chap:future-work} outlines stronger mitigations such as provenance tracking, allowlisting, and retrieval sanitization.
