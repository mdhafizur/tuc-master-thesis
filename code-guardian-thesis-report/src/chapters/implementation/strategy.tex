\section{Analysis Modes and Orchestration}
\label{sec:impl-modes}

Code Guardian supports multiple analysis modes that trade off latency, context breadth, and user disruption. Rather than implementing multiple independent pipelines, the system follows a single orchestration pattern: extract code context $\rightarrow$ (optional) retrieve security knowledge $\rightarrow$ run local LLM analysis $\rightarrow$ render diagnostics and optional fixes.

\subsection{Scopes: Function, Selection, File, Workspace}
\label{subsec:impl-scopes}

\textbf{Function scope (real-time).} During editing, Code Guardian analyzes the enclosing function at the cursor position. This reduces prompt size and supports low-latency feedback. A size threshold is applied to skip unusually large functions to avoid blocking the editor.

\textbf{Selection scope.} Developers can explicitly analyze selected code (or the current line if no selection exists). This mode is used for focused investigation and interactive follow-up.

\textbf{File scope.} Full-file analysis runs on demand and produces a complete set of findings for the document. This mode can tolerate higher latency but provides broader context for classification and repair suggestions.

\textbf{Workspace scope.} Workspace scanning analyzes multiple files and aggregates findings into the dashboard. The output is intended for prioritization (which files and categories dominate) rather than line-by-line interaction.

\subsection{LLM-Only vs.\ RAG-Enhanced Execution}
\label{subsec:impl-rag-toggle}

The system offers two operational configurations:
\begin{itemize}
  \item \textbf{LLM-only:} prompt contains the analyzed code plus strict JSON-output constraints. This configuration minimizes overhead and is suitable for real-time use.
  \item \textbf{LLM+RAG:} the prompt is augmented with retrieved CWE/OWASP/CVE knowledge snippets and secure coding guidance. This improves grounding and can reduce false positives or unsupported suggestions, at the cost of additional embedding and retrieval time.
\end{itemize}

RAG can be toggled at runtime through settings and is surfaced via a status-bar indicator in the IDE.

\subsection{Caching and Responsiveness}
\label{subsec:impl-caching}

To reduce repeated inference calls, Code Guardian caches analysis results keyed by (code snippet, model). Under typical editing patterns, small changes frequently return to previously seen states (undo/redo, formatting), making caching effective. Debouncing and caching together aim to keep real-time diagnostics responsive (R6) while maintaining stable behavior across repeated runs (R1).

\paragraph{Cache invalidation strategy.}
The cache uses a hybrid eviction policy combining time-to-live (TTL), least-recently-used (LRU) ordering, and content-based hashing to balance memory usage and hit rate:

\begin{lstlisting}[caption={Cache invalidation and eviction strategy}, label={lst:impl-cache-invalidation}]
class AnalysisCache:
  maxSize: number = 500
  ttlMs: number = 30 * 60 * 1000  // 30 minutes
  entries: Map<string, CacheEntry> = {}
  lruQueue: string[] = []  // ordered by access time

  get(key: string):
    entry = entries[key]
    if entry == null:
      return null  // cache miss

    // Check TTL expiration
    age = now() - entry.timestamp
    if age > ttlMs:
      remove(key)
      return null  // expired entry

    // Update LRU position (move to end)
    lruQueue.remove(key)
    lruQueue.push(key)

    return entry.result

  put(key: string, result: AnalysisResult):
    // Evict expired entries first
    removeExpiredEntries()

    // Evict LRU entries if at capacity
    while entries.size >= maxSize:
      oldestKey = lruQueue.shift()  // remove first
      delete entries[oldestKey]

    // Insert new entry
    entries[key] = {
      result: result,
      timestamp: now()
    }
    lruQueue.push(key)

  computeKey(code: string, model: string, ragEnabled: bool):
    // Content-based hash to detect unchanged code
    codeHash = sha256(code.trim())
    return `${codeHash}-${model}-${ragEnabled}`

  removeExpiredEntries():
    currentTime = now()
    for key in entries.keys():
      age = currentTime - entries[key].timestamp
      if age > ttlMs:
        delete entries[key]
        lruQueue.remove(key)

  clear():
    entries = {}
    lruQueue = []
\end{lstlisting}

\paragraph{Cache effectiveness.}
The content-based key (\texttt{sha256(code.trim())}) ensures that only semantic code changes invalidate the cache. Formatting changes (whitespace, indentation) that do not affect code meaning will still produce cache hits. The 30-minute TTL prevents stale results from persisting indefinitely, while the 500-entry limit prevents unbounded memory growth during long editing sessions. In evaluation runs, cache hit rates exceeded 60\% during typical editing workflows (undo/redo, minor edits, navigation).

\subsection{Cost-Aware Mode Selection}
\label{subsec:impl-cost-aware-selection}

Mode selection in Code Guardian is intentionally tied to interaction cost. Real-time diagnostics prioritize bounded latency, so they use smaller scopes and stricter guards. On-demand and workspace workflows tolerate higher latency in exchange for broader context.

In practice, the extension follows an implicit decision policy:
\begin{itemize}
  \item if interaction is continuous (typing), prefer function scope and minimal prompt overhead;
  \item if interaction is explicit (command-triggered), allow larger scopes and richer context;
  \item if repository-wide insight is needed, run bounded batch scans with aggregation.
\end{itemize}

This policy is not a static rulebook; it is a practical control layer that maps developer intent to an appropriate latency/coverage profile. It also reduces the risk of ``always-on heavy analysis'' that would otherwise degrade IDE usability.

\subsection{Guardrail Ordering and Failure Containment}
\label{subsec:impl-guardrail-order}

Guardrails are applied in a fixed order to fail fast and preserve responsiveness:
\begin{enumerate}
  \item scope extraction and size validation,
  \item cache lookup,
  \item optional retrieval preparation,
  \item model invocation with timeout/retry bounds,
  \item strict parse/validation and diagnostic rendering.
\end{enumerate}

Ordering matters operationally. Early rejection of oversized scopes avoids unnecessary retrieval and inference work, and early cache hits bypass the most expensive stage entirely. This containment strategy is one of the main reasons the extension remains usable on commodity developer hardware.

\subsection{Repair Suggestion Handling}
\label{subsec:impl-repair}

When the model returns a suggested fix, Code Guardian exposes it as a quick fix action. The extension does not auto-apply modifications; instead it requires explicit user confirmation and integrates with the editor undo stack. This design ensures that repair suggestions remain \emph{advisory} and that final responsibility for functional correctness stays with the developer (R4).
