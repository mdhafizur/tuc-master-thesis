\section{System Workflow}
\label{sec:system-workflow}

This section explains how Code Guardian turns JavaScript/TypeScript code in Visual Studio Code into vulnerability findings and repair suggestions. The workflow implements the design from Chapter~\ref{chap:concept} and ties it to the requirements in Chapter~\ref{chap:analysis}, especially privacy-preserving operation (R5), responsiveness (R6), and explainability (R3).

\subsection*{End-to-End Processing Flow}

Code Guardian exposes multiple analysis triggers (real-time and explicit commands). Two execution paths are particularly important in the prototype:
(i) a \emph{structured diagnostics} path that produces JSON findings suitable for editor diagnostics, and
(ii) an \emph{interactive analysis} path that produces Markdown explanations in a WebView and supports follow-up questions.

\begin{enumerate}
    \item \textbf{Trigger and scope selection}: An analysis run is triggered either (i) automatically while the user edits code (debounced), or (ii) explicitly via a command (analyze selection, analyze file, scan workspace). The selected scope determines how much code context is included (function vs.\ selection vs.\ full document vs.\ workspace batch).

    \item \textbf{Code context extraction}: The extension extracts the relevant code fragment and its location (start line offset). For real-time diagnostics, the default unit is the \emph{enclosing function} at the cursor position. For selection analysis, the unit is the selected region (or the current line if no selection exists). For file analysis, the entire document text is used.

    \item \textbf{Optional retrieval (RAG)}: If RAG is enabled and initialized, the system retrieves security knowledge snippets relevant to the analyzed code. Retrieval uses a local embedding model and a local vector index; retrieved guidance is appended to prompts to ground explanations and recommendations. In the current prototype, RAG is primarily used in the interactive WebView workflows and can be used for batch scanning; the real-time diagnostics path is kept lightweight to preserve responsiveness.

    \item \textbf{LLM analysis (local)}: The local model (Ollama) is invoked with different output constraints depending on the workflow.
    \begin{itemize}
        \item \textbf{Structured diagnostics}: the model is required to return \emph{only} a JSON array of issues (message, line range, optional fix). Light post-processing removes Markdown fences and extracts the first JSON array substring if needed.
        \item \textbf{Interactive analysis}: the model streams Markdown-formatted explanations to a WebView, enabling richer narratives and follow-up questions.
    \end{itemize}

    \item \textbf{Diagnostics and UI rendering}: Findings are converted into VS Code diagnostics and rendered inline (squiggles), in the Problems panel, and via hover tooltips. When a suggested fix is available, a quick-fix code action is offered so the developer can apply a patch after review.

    \item \textbf{Caching and statistics}: Results are cached by (code snippet, model) to avoid repeated inference. The cache uses LRU-style eviction and a time-to-live policy, and cache statistics can be inspected from the UI to validate responsiveness improvements under repeated edits.
\end{enumerate}

\subsection*{Deterministic Orchestration Contract}

Although model inference is probabilistic, orchestration in the extension is intentionally deterministic. For a fixed input snapshot, active model, and prompt mode, the non-model part of the pipeline (scope extraction, prompt assembly structure, parsing, localization mapping, and diagnostic emission) follows a fixed sequence with explicit guard conditions. This reduces operational variance and isolates model effects during debugging and evaluation.

The runtime control flow can be summarized as:
\begin{lstlisting}[caption={Simplified orchestration logic for diagnostics flow}, label={lst:impl-orchestration}]
onTrigger(event):
  scope = extractScope(event, mode)
  if scope.isEmpty or scope.size > scopeLimit: return

  key = hash(scope.text, model, ragEnabled)
  if cache.hasFresh(key):
    return render(cache.get(key), scope.offset)

  prompt = buildPrompt(scope.text, ragContextIfEnabled)
  raw = callLocalModel(prompt, timeout, retries)
  issues = parseAndValidateJsonArray(raw)
  mapped = mapSnippetLinesToDocument(issues, scope.offset)

  cache.put(key, mapped)
  render(mapped, scope.offset)
\end{lstlisting}

\subsection*{Cost and Latency Drivers}

In practice, end-to-end latency is driven by a small set of controllable factors:
\begin{itemize}
  \item \textbf{Scope size}: larger snippets increase prompt length and token-generation time.
  \item \textbf{Model profile}: higher-capacity models generally improve reasoning capacity at the cost of slower response.
  \item \textbf{RAG context volume}: retrieval adds embedding/search overhead and prompt expansion.
  \item \textbf{Cache hit rate}: repeated edits over similar snippets can avoid most model calls.
\end{itemize}

This can be interpreted with a simple additive model:
\[
T_{\text{e2e}} \approx T_{\text{extract}} + T_{\text{retrieve}} + T_{\text{infer}} + T_{\text{parse/map}} + T_{\text{render}},
\]
where \(T_{\text{retrieve}}\) is near zero in LLM-only mode and \(T_{\text{infer}}\) dominates in most non-cached calls. The implementation choices in this chapter primarily target the dominant terms (\(T_{\text{infer}}\) and, when enabled, \(T_{\text{retrieve}}\)).

\subsection*{Interactive vs.\ Batch Workflows}

\textbf{Real-time workflow (function-level).} When editing, Code Guardian runs on a small scope and uses debouncing to avoid overwhelming the local model. A size threshold skips unusually large functions (2000 characters) to bound worst-case latency. This mode is optimized for R6 and is intended to surface common, localized patterns (e.g., injection sinks, insecure randomness) with minimal disruption.

\textbf{On-demand workflow (selection/file).} The developer can explicitly request analysis of selected code (interactive WebView) or the full file (structured diagnostics). Full-file analysis includes a conservative size guard (20{,}000 characters) to avoid excessively large prompts.

\textbf{Workspace workflow (dashboard scan).} The workspace scanner iterates through JavaScript/TypeScript files (excluding \texttt{node\_modules}) and aggregates findings into a dashboard. Very large files are skipped (500\,KB) to keep scans bounded. This supports risk overview and prioritization by surfacing the most affected files and a coarse severity distribution.

\subsection*{User-Controlled Remediation}

Repair suggestions are never applied automatically. Instead, the system presents a quick-fix option that inserts the suggested patch only after explicit user confirmation. This design preserves developer control (R4), prevents silent behavior changes, and keeps responsibility for functional correctness with the human developer.
