In contrast to much prior work, this thesis focuses on privacy-preserving vulnerability detection and repair using locally deployed LLMs integrated into Visual Studio Code. The proposed system combines retrieval-augmented reasoning with IDE-level context extraction to support both real-time and on-demand security analysis for JavaScript and TypeScript codebases. Unlike fine-tuned or cloud-dependent approaches, the system runs on-device by default and can operate fully offline when knowledge refresh is disabled. It also decouples security knowledge from model parameters and is evaluated through reproducible local ablation experiments (LLM-only vs.\ LLM+RAG) with quantitative metrics.

The core positioning claim is not that local LLMs outperform all alternatives across all metrics, but that they enable a \emph{different deployment envelope}: privacy-preserving IDE assistance with explicit control over model choice, retrieval strategy, and output contracts. This makes the approach operationally distinct from cloud assistants and complements pure SAST pipelines rather than replacing them.

Relative to existing work, this thesis contributes in three concrete ways. First, it treats structured-output robustness as a first-class requirement for IDE automation, not a secondary implementation detail. Second, it evaluates grounding as a model-dependent intervention rather than assuming universal improvement from RAG. Third, it reports a baseline comparison with requested SAST tools under the same local evaluation harness, enabling direct interpretation of quality-versus-noise trade-offs in a shared setup.

By addressing detection consistency, contextual reasoning, explainability, actionable repair, privacy preservation, and usability within a unified framework, this work aims to bridge the gap between academic advances in LLM-based security analysis and the practical requirements of real-world software development workflows.
