Static Application Security Testing (SAST) tools remain the default workhorse for vulnerability detection in many development workflows. Rule-based analyzers and taint-analysis systems such as Semgrep and CodeQL statically inspect source code for predefined patterns and data-flow queries \cite{semgrepDocs,codeqlDocs}. Their main advantage is operational: results are deterministic, reproducible, and easy to integrate into CI/CD pipelines and compliance-driven environments.

From a research perspective, modern static analyzers build on foundational ideas such as abstract interpretation \cite{cousot1977abstract} and scalable bug-finding techniques \cite{engler2001bugs}. In practice, large-scale deployments demonstrate that static analysis can find real defects in industrial codebases at massive scale \cite{bessey2010billion}. Security-oriented static analysis has also been studied explicitly, including work that frames static analysis as an effective security engineering control when combined with secure development processes \cite{chess2004staticanalysis,howard2006sdl,mcgraw2006softwaresecurity}.

A second advantage of SAST is \emph{auditability}. Findings are tied to explicit rules or queries, which enables traceable reasoning, stable regression behavior, and predictable security gates. This is particularly relevant in regulated settings, where organizations need evidence of controls rather than only aggregate accuracy claims. Determinism also simplifies governance: when a rule is updated, teams can review the diff in findings directly instead of inferring changes from opaque model behavior.

These strengths come with well-known limitations. SAST tools can struggle with contextual reasoning and generalization, producing false positives when a risky-looking pattern is guarded elsewhere (e.g., validation in a different function, framework-enforced invariants) and false negatives when vulnerabilities depend on semantic relationships or framework-specific behavior. Language and framework abstractions further complicate exhaustive rule coverage and often push teams toward conservative, noisy rulesets.

There is also a structural precision/recall tension in static analysis deployments. Aggressive rule sets can improve vulnerable-case coverage, but often increase triage burden through noisy alerts; stricter rules reduce noise but risk under-detection. The trade-off is not only technical but organizational: teams with limited security-review capacity may prefer conservative rule sets, while high-assurance environments may tolerate larger alert queues to avoid misses.

Finally, many SAST tools provide limited remediation guidance, requiring developers to interpret findings and design fixes manually. Empirical studies show that warning overload and poor actionability reduce adoption and remediation rates \cite{johnson2013don,christakis2016developers}. This gap between \emph{detection} and \emph{remediation support} is one reason why SAST outputs are often strongest as gatekeeping signals, but weaker as day-to-day developer coaching tools.

These limitations motivate approaches that keep deterministic checks as guardrails while adding more flexible reasoning and explanation generation closer to the developer workflow. In this thesis, Code Guardian follows this complementary idea: it preserves IDE diagnostics and baseline SAST comparison, while using grounded LLM reasoning to improve explanation quality and provide developer-controlled repair suggestions.
