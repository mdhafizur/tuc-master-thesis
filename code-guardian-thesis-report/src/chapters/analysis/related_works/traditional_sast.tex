Static Application Security Testing (SAST) tools have long been the primary means of detecting vulnerabilities during development. Rule-based analyzers and taint-analysis systems, such as Semgrep and CodeQL, identify predefined vulnerability patterns by statically inspecting source code \cite{semgrepDocs,codeqlDocs}. These tools provide deterministic and reproducible results, which makes them suitable for automated pipelines and compliance-driven environments.

From a research perspective, modern static analyzers build on foundational ideas such as abstract interpretation \cite{cousot1977abstract} and scalable bug-finding techniques \cite{engler2001bugs}. In practice, large-scale deployments demonstrate that static analysis can find real defects in industrial codebases at massive scale \cite{bessey2010billion}. Security-oriented static analysis has also been studied explicitly, including work that frames static analysis as an effective security engineering control when combined with secure development processes \cite{chess2004staticanalysis,howard2006sdl,mcgraw2006softwaresecurity}.

An important practical advantage of SAST is \emph{auditability}. Findings are typically tied to explicit rules or data-flow queries, which enables traceable reasoning and predictable regression behavior in CI/CD usage. This is particularly relevant in regulated settings, where organizations need stable controls and clear evidence for security gates. Determinism also simplifies governance: when a rule is updated, the effect on findings can be reviewed directly, instead of being mediated by opaque model behavior.

Despite these strengths, SAST tools can struggle with contextual reasoning and generalization. They frequently produce false positives when security-relevant patterns appear in benign contexts (e.g., input validated elsewhere, framework-enforced invariants) and may miss vulnerabilities that depend on semantic relationships or framework-specific behavior. In practice, these errors are amplified by language- and framework-level abstractions that are difficult to model exhaustively with static rules.

There is also a structural precision/recall tension in static analysis deployments. Aggressive rule sets can improve vulnerable-case coverage, but often increase triage burden through noisy alerts; stricter rules reduce noise but risk under-detection. The trade-off is not only technical but organizational: teams with limited security-review capacity may prefer conservative rule sets, while high-assurance environments may tolerate larger alert queues to avoid misses.

Moreover, many SAST tools offer limited remediation guidance, requiring developers to interpret findings and manually design fixes. Empirical studies show that warning overload and poor actionability reduce adoption and remediation rates \cite{johnson2013don,christakis2016developers}. This gap between \emph{detection} and \emph{remediation support} is one reason why SAST outputs are often strongest as gatekeeping signals, but weaker as day-to-day developer coaching tools.

These limitations motivate research into approaches that complement deterministic static analysis with more flexible reasoning and explanation generation. In this thesis, Code Guardian aims to preserve the determinism and localization benefits of IDE diagnostics while using LLM-based reasoning (grounded by security knowledge) to improve explanation quality and provide developer-controlled repair suggestions.
