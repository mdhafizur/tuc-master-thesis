Retrieval-Augmented Generation (RAG) is a general paradigm for grounding language model outputs in external knowledge without retraining. In RAG, a retriever selects relevant documents for a given query and the generator conditions on those documents when producing an answer \cite{lewis2020rag}. Retrieval can be implemented with lexical scoring functions such as BM25 \cite{robertson2009bm25} or with dense retrieval based on semantic embeddings. Dense retrieval approaches such as DPR and retrieval-augmented pretraining (REALM) motivate this design by showing that semantic retrieval can provide effective context for generation \cite{karpukhin2020dpr,guu2020realm}. Survey work further highlights RAG as a practical way to reduce hallucination and improve factuality \cite{mialon2023augmented,ji2023hallucination}.

In secure coding settings, the retrieved context typically corresponds to vulnerability taxonomies (e.g., CWE), secure coding guidelines (e.g., OWASP cheat sheets), and historical vulnerability examples. This fits vulnerability detection and repair well because many issues are best explained by mapping code patterns to known weakness classes and established mitigations \cite{mitreCWE,owaspCheatSheets,owaspTop10_2021}. By grounding the model in such sources, a system can improve consistency (R1) and explanation quality (R3), while reducing unsupported guesses.

Nevertheless, RAG introduces its own failure modes. If retrieval returns weakly related snippets, the generator can become distracted and produce confident but misaligned explanations. If retrieval returns overly generic security text, outputs may drift toward checklist-style warnings that increase false positives. In security tooling, retrieval quality is therefore as important as model quality: grounding helps only when retrieved context is both relevant and specific to the analyzed code pattern.

Implementation-wise, RAG depends on embedding models and efficient nearest-neighbor search. Sentence-level embedding methods such as Sentence-BERT are commonly used to map text into vector space \cite{reimers2019sentencebert}. Approximate nearest-neighbor indices such as HNSW provide high recall with practical latency \cite{malkov2018hnsw}, and libraries such as FAISS popularized large-scale similarity search in practice \cite{johnson2017faiss}. In Code Guardian, these ideas are realized through local embeddings and a persistent HNSW vector store to keep retrieval on-device and compatible with IDE latency constraints.

From an engineering perspective, RAG design involves a three-way trade-off among latency, grounding depth, and noise:
\begin{itemize}
  \item increasing top-$k$ retrieved chunks can improve recall of relevant guidance but expands prompt size and latency;
  \item larger chunks preserve context but may dilute precision in similarity search;
  \item aggressive knowledge refresh increases topicality but can introduce quality variance and reduce reproducibility if source curation is weak.
\end{itemize}

These trade-offs motivate model-specific calibration rather than one global RAG configuration. The same retrieval setup can improve one model while degrading another, depending on how each model integrates external context under strict output constraints. This observation is central to the ablation design in this thesis.
