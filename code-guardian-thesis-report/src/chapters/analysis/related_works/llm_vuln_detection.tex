Recent advances in Large Language Models (LLMs) have prompted extensive investigation into their use for code understanding, vulnerability detection, and repair suggestion. Contemporary models build on transformer architectures \cite{vaswani2017attention} and are often trained with large-scale pretraining objectives in the style of BERT/T5 \cite{devlin2019bert,raffel2020t5}. General-purpose LLMs and code-specialized models can generate syntactically plausible code and perform transformations such as refactoring and patch drafting \cite{brown2020gpt3,chen2021codex,openai2023gpt4,feng2020codebert,wang2021codet5,nijkamp2022codegen}. These capabilities make LLMs attractive for IDE-integrated security assistance, where developers benefit from explanations and candidate fixes at the point of code.

However, empirical evaluations show that model-generated code can be \emph{functionally correct yet insecure}, and that probabilistic generation can introduce hallucinated assumptions or omit critical security checks \cite{pearce2022copilot,ji2023hallucination,bender2021stochastic}. This is particularly problematic for security, where small omissions (e.g., missing validation, unsafe sinks) can create exploitable weaknesses.

A key difference from classical static analysis is that LLM behavior is strongly prompt- and format-dependent. Small changes in instructions, output schema, or contextual examples can materially change both finding rates and false-positive behavior. As a result, measured model quality is not only a property of model weights, but also of orchestration choices such as scope selection, schema strictness, and failure handling.

Before LLMs, machine-learning-based vulnerability detection explored learning semantic representations of code for classification. Early systems used deep learning over code fragments and patterns \cite{li2018vuldeepecker}, while representation-learning work explored embeddings for code structure and semantics \cite{alon2019code2vec}. More recent approaches leveraged graph representations to capture richer program semantics \cite{zhou2019devign,allamanis2018learninggraphs}. These methods improved over purely lexical baselines, but often required task-specific training data and did not naturally provide developer-facing explanations or repair suggestions.

Finally, security-related failure modes of LLMs extend beyond simple false positives/negatives. The broader LLM risk literature emphasizes both ethical/operational risks \cite{weidinger2021ethical} and adversarial behaviors such as jailbreaks and prompt-based attacks \cite{zou2023universal}. For IDE-integrated security tools, these risks motivate strict output contracts, careful prompt design, and conservative integration of model suggestions into developer workflows.

Another practical challenge is \emph{calibration}. Many LLM-based assistants can explain a vulnerability convincingly even when the underlying label is wrong or weakly grounded. Without explicit confidence controls and abstention behavior, this can increase developer over-trust. For security tooling, persuasive explanations are not sufficient; the system must also provide stable structure, traceable evidence, and clear boundaries between high-confidence findings and uncertain hypotheses.

In response, contemporary approaches increasingly combine learned models with auxiliary signals such as retrieval, tools, or deterministic checks. Retrieval-augmented generation provides a mechanism to ground model reasoning in external security knowledge without retraining \cite{lewis2020rag,karpukhin2020dpr,guu2020realm}. Tool-oriented prompting (e.g., using dedicated retrieval or analysis tools) further motivates modular designs where different components provide complementary evidence \cite{schick2023toolformer}. In Code Guardian, these ideas are reflected in the optional RAG module and the strict JSON-output contract that supports IDE diagnostics.

Repair suggestion quality is also connected to the broader literature on automated program repair. Classic systems such as GenProg and subsequent patch-learning approaches show both the promise of automation and the difficulty of evaluating patch correctness beyond passing tests \cite{weimer2009genprog,legoues2012genprog,kim2013par,monperrus2014critique}. For IDE-integrated security assistance, these findings motivate presenting repairs as \emph{optional} quick fixes, requiring explicit developer review and preserving responsibility for functional correctness.
