\label{sec:r6-usability}

Usability in the context of the proposed framework refers primarily to \textbf{latency}, defined as the end-to-end time delay between a developer action and the presentation of security feedback within the Integrated Development Environment (IDE). This includes the time required for context extraction, model inference, retrieval-augmented reasoning, and rendering of vulnerability findings or repair suggestions. While the system supports multiple interaction modes, the core task is the transformation of \emph{source code input} into \emph{actionable security feedback}. Accordingly, the latency requirement applies uniformly across inline detection, on-demand analysis, and repair suggestion workflows.

Responsiveness directly determines whether security assistance can be integrated naturally into the development process. Human–computer interaction research consistently shows that feedback delivered within a few seconds preserves a sense of flow and supports effective turn-taking, whereas longer delays disrupt concentration and reduce tool adoption \cite{card1991model, nielsen1994usability}. In the context of IDE-based development, developers expect near-immediate feedback comparable to other static diagnostics such as type errors or linting warnings. Excessive latency risks relegating security analysis to a background task that is ignored or deferred.

% As illustrated in Figure~\ref{fig:ide-interaction-example}, vulnerability
Vulnerability annotations and repair suggestions should appear promptly after a triggering event, such as saving a file or explicitly invoking an analysis command. Timely feedback enables developers to assess security implications while the relevant code context is still active, reducing cognitive load and improving remediation efficiency.

Unlike other requirements—such as detection accuracy (R1), contextual reasoning (R2), explainability (R3), repair quality (R4), or privacy preservation (R5)—usability in this thesis is scoped strictly to latency. This focus reflects the practical reality that even accurate and well-explained security findings are unlikely to be acted upon if they arrive too late to fit within normal development workflows. This concern is particularly relevant for local LLM-based systems, where inference time can be substantial compared to traditional static analysis.

Latency, however, is not an absolute property of the system alone. It is strongly influenced by the \textbf{hardware and deployment environment} on which the system operates. Dedicated accelerators such as GPUs can significantly reduce inference time, whereas CPU-only or resource-constrained environments typically incur higher latency. Additionally, factors such as model size, retrieval depth, and concurrency affect responsiveness. For this reason, all latency measurements must be reported together with the corresponding hardware profile, including processor type, available memory, and accelerator configuration. This ensures that usability claims are interpreted relative to realistic deployment scenarios rather than as hardware-agnostic performance guarantees.

For evaluation in this thesis run, usability is operationalized with \textbf{median} end-to-end latency (typical interaction cost) and \textbf{mean} latency (slow-tail sensitivity proxy) across representative interaction scenarios. This matches the current harness output and avoids overstating p95 claims that are not directly exported in the reported artifacts. Separate latency measurements are still reported for interactive inline detection and more comprehensive explicitly triggered analyses.

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.6}
\setlength{\tabcolsep}{12pt}
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}m{3cm}|>{\arraybackslash}X|}
\hline
\textbf{Visual Score} & \textbf{Interpretation (median/mean criteria)} \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} 
& \textbf{High usability.} Median end-to-end latency $\leq$ 2\,s and mean latency $\leq$ 3\,s for inline detection on the declared hardware profile. Interaction remains fluid and non-disruptive. \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm) arc (90:-90:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}}
& \textbf{Medium usability.} Median latency in $(2, 5)$\,s or mean latency in $(3, 6)$\,s. Delay is noticeable but acceptable for non-continuous security feedback. \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} 
& \textbf{No usability.} Median latency $>$ 5\,s, mean latency $>$ 6\,s, or frequent timeouts in inline scenarios. Feedback is too slow for practical integration into the development workflow. \\
\hline
\end{tabularx}
\caption{Evaluation scale for R6: Usability (Latency, median/mean operationalization).}
\label{tab:r6-usability}
\end{table}

In this thesis, usability is evaluated primarily through responsiveness under realistic hardware constraints. Reporting latency together with environment details keeps performance claims comparable and interpretable.
