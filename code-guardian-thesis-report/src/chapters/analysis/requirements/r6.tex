\label{sec:r6-usability}

Usability in the context of the proposed framework refers primarily to \textbf{latency}, defined as the end-to-end time delay between a developer action and the presentation of security feedback within the Integrated Development Environment (IDE). This includes the time required for context extraction, model inference, retrieval-augmented reasoning, and rendering of vulnerability findings or repair suggestions. While the system supports multiple interaction modes, the core task is the transformation of \emph{source code input} into \emph{actionable security feedback}. Accordingly, the latency requirement applies uniformly across inline detection, on-demand analysis, and repair suggestion workflows.

Responsiveness directly determines whether security assistance can be integrated naturally into the development process. Human–computer interaction research consistently shows that feedback delivered within a few seconds preserves a sense of flow and supports effective turn-taking, whereas longer delays disrupt concentration and reduce tool adoption \cite{card1991model, nielsen1994usability}. In the context of IDE-based development, developers expect near-immediate feedback comparable to other static diagnostics such as type errors or linting warnings. Excessive latency risks relegating security analysis to a background task that is ignored or deferred.

% As illustrated in Figure~\ref{fig:ide-interaction-example}, vulnerability
Vulnerability annotations and repair suggestions should appear promptly after a triggering event, such as saving a file or explicitly invoking an analysis command. Timely feedback enables developers to assess security implications while the relevant code context is still active, reducing cognitive load and improving remediation efficiency.

Unlike other requirements—such as detection accuracy (R1), contextual reasoning (R2), explainability (R3), repair quality (R4), or privacy preservation (R5)—usability in this thesis is scoped strictly to latency. This focus reflects the practical reality that even accurate and well-explained security findings are unlikely to be acted upon if they arrive too late to fit within normal development workflows. This concern is particularly relevant for local LLM-based systems, where inference time can be substantial compared to traditional static analysis.

Latency, however, is not an absolute property of the system alone. It is strongly influenced by the \textbf{hardware and deployment environment} on which the system operates. Dedicated accelerators such as GPUs can significantly reduce inference time, whereas CPU-only or resource-constrained environments typically incur higher latency. Additionally, factors such as model size, retrieval depth, and concurrency affect responsiveness. For this reason, all latency measurements must be reported together with the corresponding hardware profile, including processor type, available memory, and accelerator configuration. This ensures that usability claims are interpreted relative to realistic deployment scenarios rather than as hardware-agnostic performance guarantees.

For evaluation, usability is measured using the 95th percentile end-to-end (p95 E2E) latency across representative interaction scenarios. The p95 metric captures worst-case responsiveness experienced by users while remaining robust to isolated outliers. Separate latency measurements are reported for interactive inline detection and for more comprehensive, explicitly triggered analyses.

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.6}
\setlength{\tabcolsep}{12pt}
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}m{3cm}|>{\arraybackslash}X|}
\hline
\textbf{Visual Score} & \textbf{Interpretation} \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} 
& \textbf{High usability.} p95 end-to-end latency $\leq$ 2\,s for inline detection on the declared hardware profile. Interaction remains fluid and non-disruptive. \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm) arc (90:-90:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}}
& \textbf{Medium usability.} p95 end-to-end latency in $(2, 5)$\,s. Delay is noticeable but acceptable for security feedback that is not continuously triggered. \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} 
& \textbf{No usability.} p95 end-to-end latency $>$ 5\,s for inline scenarios or frequent timeouts. Feedback is too slow for practical integration into the development workflow. \\
\hline
\end{tabularx}
\caption{Evaluation scale for R6: Usability (Latency).}
\label{tab:r6-usability}
\end{table}

In summary, usability in this framework is defined as responsiveness measured through end-to-end latency for IDE-integrated vulnerability detection and repair. Because latency is inherently dependent on hardware and deployment conditions, all usability evaluations must be contextualized by reporting the corresponding execution environment. This ensures that results are comparable, interpretable, and grounded in realistic usage scenarios.
