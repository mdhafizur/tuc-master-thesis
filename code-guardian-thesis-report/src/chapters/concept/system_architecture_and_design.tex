\section{System Architecture and Design}
\label{sec:system-architecture}

This section presents the high-level architecture of Code Guardian using C4-style views (context, containers, and process). The goal is to make the privacy boundary and the main data flows explicit: code stays on-device, local inference is performed through a localhost LLM backend, and retrieval (when enabled) is backed by a local knowledge base and vector index.

\subsection{Context View: System Boundary and External Interactions}
\label{subsec:context-view}

Figure~\ref{fig:c4-context} positions Code Guardian within its operational environment. The primary actor is the developer working inside Visual Studio Code. The system executes in the VS Code extension host and communicates with a local LLM backend (Ollama) running on the same machine \cite{ollamaDocs}.

\textbf{Local assets.} The core local assets are:
\begin{itemize}
    \item \textbf{Workspace source code} (JavaScript/TypeScript) being edited.
    \item \textbf{Local LLM runtime} (Ollama) used for analysis and (optionally) embeddings.
    \item \textbf{Local security knowledge base} and \textbf{vector index} used for retrieval when RAG is enabled \cite{langchainDocs,malkov2018hnsw}.
    \item \textbf{Local caches} for analysis results and vulnerability metadata.
\end{itemize}

\textbf{Optional external data.} Code Guardian may optionally fetch \emph{public vulnerability metadata} to refresh the knowledge base (e.g., CVE records from the NVD API). This traffic does not include user source code and can be disabled by running in offline mode. After a refresh, cached knowledge can be reused without network access. This separation preserves the core privacy goal (no source-code exfiltration) while still allowing the knowledge base to evolve over time.

\begin{figure}[H]
  \centering
  \safeincludegraphics[width=1\linewidth]{images/c4_context.drawio.pdf}
  \caption{C4 Context diagram showing system boundary and external interactions. Code Guardian (blue box) operates as a VS Code extension with local backend. Developer interacts with VS Code, which triggers analysis. Code Guardian communicates with local Ollama LLM (localhost:11434), local vector database, and optionally fetches public metadata from CVE/NVD APIs (dashed gold). Privacy boundary (green dashed line): all source code analysis stays on localhost.}
  \label{fig:c4-context}
\end{figure}

\subsection{Container View: Internal Component Structure}
\label{subsec:container-view}

Figure~\ref{fig:c4-container} summarizes the main internal containers and their responsibilities.

\textbf{VS Code extension host.} The extension is responsible for registering editor triggers, extracting analysis scopes, and rendering results using VS Code diagnostics and WebViews \cite{vscodeExtensionApi}. It provides commands for file analysis, selection analysis, contextual Q\&A, model selection, cache inspection, and workspace scanning.

\textbf{Structured diagnostics pipeline.} For real-time and file-level diagnostics, the extension invokes a JSON-only analyzer that returns a list of issues with line ranges and optional fixes. These results are mapped into VS Code diagnostics and quick fixes. A bounded analysis cache reduces redundant LLM calls for unchanged snippets.

\textbf{Interactive analysis pipeline.} For selection analysis and contextual Q\&A, the extension opens a WebView panel and streams Markdown-formatted responses from the local model. This mode supports conversational follow-up and can incorporate retrieved knowledge when RAG is enabled.

\textbf{RAG manager and data manager.} When enabled, the RAG manager maintains a local knowledge base (serialized entries) and a persistent vector store. A vulnerability data manager refreshes public metadata (CWE-/OWASP-aligned curated entries and optional CVE/advisory sources) and caches results on disk. The vector store is rebuilt or updated based on the knowledge base content.

\begin{sidewaysfigure}
  \centering
  \safeincludegraphics[width=1\linewidth]{images/c4_container.drawio.pdf}
  \caption{C4 Container diagram showing internal components. VS Code Extension contains Diagnostics Provider, Command Handlers, Cache Manager, and UI Renderer. Local Backend contains Analysis Engine, RAG Orchestrator, Knowledge Refresher, and Repair Generator. Data containers include Vector Database (CWE/OWASP/CVE embeddings) and Result Cache. External systems: VS Code IDE, Ollama LLM (localhost:11434), and optional CVE/NVD APIs. All source code stays within system boundary.}
  \label{fig:c4-container}
\end{sidewaysfigure}

\subsection{Process View: End-to-End Workflows}
\label{subsec:process-view}

Figure~\ref{fig:bpmn} illustrates the dynamic execution flow for the main workflows.

\textbf{Real-time diagnostics (function scope).}
\begin{enumerate}
    \item A JavaScript/TypeScript document change event occurs in the active editor.
    \item After a debounce interval (800\,ms), the extension extracts the innermost enclosing function at the cursor.
    \item If the extracted scope is within size limits, the local analyzer is invoked and required to return a JSON array of findings.
    \item Findings are parsed defensively, cached, mapped to document ranges, and rendered as diagnostics. Optional \texttt{suggestedFix} strings are surfaced as quick fixes.
\end{enumerate}

\textbf{On-demand file diagnostics.}
\begin{enumerate}
    \item The developer invokes the ``analyze full file'' command.
    \item The full document text is analyzed (subject to a size guard).
    \item Findings are mapped to diagnostics and rendered in the editor.
\end{enumerate}

\textbf{Interactive analysis (selection) and contextual Q\&A.}
\begin{enumerate}
    \item The developer selects code (or context files/folders) and asks a security question.
    \item The extension collects the selected context locally and opens a WebView panel.
    \item The local model streams Markdown-formatted responses; follow-up questions extend the conversation history.
    \item When enabled, retrieved security knowledge can be injected into prompts to ground explanations.
\end{enumerate}

\textbf{Workspace scan and dashboard.}
\begin{enumerate}
    \item The developer starts a workspace scan from the command palette.
    \item The scanner enumerates JS/TS files, excluding dependencies, and skips very large files.
    \item Each file is analyzed locally; issues are aggregated and summarized by severity heuristics and issue density.
    \item Results are shown in a dashboard WebView, which can open files directly and trigger rescans.
\end{enumerate}

\textbf{Privacy considerations in the process view.} Across all workflows, source code is sent only to the local LLM backend on \texttt{localhost}. Optional knowledge refresh operations fetch only public metadata and are cached; disabling refresh yields a fully offline analysis mode.

\begin{sidewaysfigure}
  \centering
  \safeincludegraphics[width=\linewidth]{images/bpmn_process_flow.pdf}
  \caption{Process view showing two primary workflows with swim lanes. Workflow 1: Real-time function analysis with debouncing (500ms) and caching---cache hits: 5--10ms, cache misses invoke LLM (1--3s). Workflow 2: On-demand analysis with optional RAG---embedding generation, vector search (top-5), prompt augmentation, LLM inference (1.5--2.5s total). Color coding: user events (yellow), extension (orange), backend (purple), LLM (green). Privacy boundary (green dashed): all processes execute on localhost.}
  \label{fig:bpmn}
\end{sidewaysfigure}

These architecture views make explicit how Code Guardian operationalizes the conceptual design: modular responsibilities, local inference as the default deployment, optional retrieval grounding, and IDE-native presentation with developer-controlled remediation.

\subsection{Sequence Diagrams: Concrete Detection Flows}
\label{sec:sequence-diagrams}

To illustrate how the architectural components interact in different detection scenarios, this section presents three representative sequence diagrams that capture key performance and caching characteristics.

\paragraph{Inline Mode with Cache Hit (Figure~\ref{fig:seq-inline-cached}).}
When a developer saves a file and the function has been previously analyzed, the extension retrieves cached results with minimal latency (5-10\,ms). This scenario demonstrates the effectiveness of content-based caching for unchanged code:
\begin{enumerate}
    \item Developer saves file.
    \item Extension extracts function context and computes content hash.
    \item Cache layer returns cached findings (no LLM invocation).
    \item Diagnostics are rendered immediately in the IDE.
\end{enumerate}
This flow is critical for IDE responsiveness, as it avoids LLM inference overhead for unmodified code.

\paragraph{Audit Mode with RAG (Figure~\ref{fig:seq-audit-rag}).}
When performing a full workspace scan with retrieval augmentation enabled, the backend orchestrates vector search and LLM generation:
\begin{enumerate}
    \item Developer triggers audit scan.
    \item Extension extracts code context.
    \item Backend generates embedding and queries vector database (top-$k=5$ chunks).
    \item Retrieved CWE/OWASP chunks are injected into the LLM prompt.
    \item Ollama performs inference (~1.5\,s for typical models).
    \item Backend parses JSON response and returns findings.
    \item Extension updates cache and renders diagnostics.
\end{enumerate}
Total latency is approximately 1.5--2\,s per function, dominated by LLM inference time. RAG overhead (embedding + retrieval) adds only ~50--100\,ms.

\paragraph{Real-time Detection with Debouncing (Figure~\ref{fig:seq-realtime-debounced}).}
During active typing, debouncing prevents analysis on every keystroke while maintaining responsiveness:
\begin{enumerate}
    \item Developer types code; each \texttt{onChange} event resets a 500\,ms timer.
    \item When typing pauses for 500\,ms, the timer expires.
    \item Extension extracts function scope and checks cache.
    \item \textbf{If cache hit:} returns cached result immediately.
    \item \textbf{If cache miss:} invokes LLM backend and stores result in cache.
    \item Diagnostics are rendered in the editor.
\end{enumerate}
Debouncing reduces unnecessary LLM invocations by 80--90\% during typical editing sessions, balancing responsiveness with resource efficiency.

\begin{table}[htbp]
\centering
\caption{Comparison of detection flow characteristics.}
\label{tab:flow-characteristics}
\begin{tabular}{llll}
\toprule
\textbf{Flow} & \textbf{Trigger} & \textbf{Latency} & \textbf{LLM Invocation} \\
\midrule
Inline (cached) & File save & 5--10\,ms & No \\
Inline (uncached) & File save & 0.3--3\,s & Yes \\
Audit + RAG & Manual scan & 1.5--2\,s & Yes (with retrieval) \\
Real-time (debounced) & Typing pause (500\,ms) & Variable & Conditional \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
  \centering
  \safeincludegraphics[width=0.9\textwidth]{images/seq_inline_cached.drawio.pdf}
  \caption{Sequence diagram: Inline mode with cache hit. No LLM invocation is required for previously analyzed code.}
  \label{fig:seq-inline-cached}
\end{figure}

\begin{figure}[htbp]
  \centering
  \safeincludegraphics[width=0.95\textwidth]{images/seq_audit_rag.drawio.pdf}
  \caption{Sequence diagram: Audit mode with RAG. Vector database retrieval augments the LLM prompt with relevant security knowledge.}
  \label{fig:seq-audit-rag}
\end{figure}

\begin{figure}[htbp]
  \centering
  \safeincludegraphics[width=0.95\textwidth]{images/seq_realtime.drawio.pdf}
  \caption{Sequence diagram: Real-time detection flow with debouncing. A 500\,ms timer prevents excessive LLM invocations during active typing.}
  \label{fig:seq-realtime-debounced}
\end{figure}
