
Secure coding support within the integrated development environment is increasingly in demand, as developers prefer immediate and actionable feedback during implementation rather than deferred security checks in later pipeline stages. Traditional static application security testing tools remain effective for rule-based vulnerabilities but exhibit limitations in detecting weaknesses that require semantic reasoning, cross-file context, or domain knowledge. Large Language Models (LLMs) offer new potential for vulnerability detection and automated repair suggestions; however, their adoption is constrained by inconsistent detection quality, high false-positive rates, outdated security knowledge, and significant privacy concerns when proprietary source code is processed by cloud-based services.

This thesis investigates a privacy-preserving approach to source code vulnerability detection and repair by integrating locally deployed LLMs with Retrieval-Augmented Generation (RAG) in a Visual Studio Code extension. The proposed system operates on-device, preserving a local source-code boundary while optionally using locally cached security knowledge that can be refreshed from curated Common Weakness Enumeration (CWE), Common Vulnerabilities and Exposures (CVE), and OWASP sources. Two operational modes are designed: a low-latency inline mode using LLM-only inference for real-time feedback, and an audit mode that combines LLM reasoning with retrieval-based context enrichment for deeper analysis.

The system is evaluated on a curated JavaScript/TypeScript evaluation set of 128 cases (113 vulnerable, 15 secure) aligned to CWE/OWASP concepts and validated against real-world CVE patterns. The quantitative comparison covers LLM-only and LLM+RAG configurations across precision, recall, F1-score, false-positive rate, JSON parse success, and latency. Results show a clear model-dependent trade-off: smaller models are faster but miss more vulnerabilities, several mid-sized configurations improve recall at the cost of high alert noise, and the best F1 score (63.76\%) is achieved by \texttt{qwen3:8b} with RAG. Overall, the study shows that privacy-preserving local LLM workflows can provide useful vulnerability analysis support, while model selection and false-positive control remain critical for practical reliability.

\medskip
\noindent\textbf{Keywords:} source code security, vulnerability detection, secure code repair, large language models, retrieval-augmented generation, privacy-preserving systems, Visual Studio Code
