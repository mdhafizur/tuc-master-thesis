
Secure coding support within the integrated development environment is increasingly demanded, as developers prefer immediate and actionable feedback during implementation rather than deferred security checks in later pipeline stages. Traditional static application security testing tools remain effective for rule-based vulnerabilities but exhibit limitations in detecting weaknesses that require semantic reasoning, cross-file context, or domain knowledge. Large Language Models (LLMs) offer new potential for vulnerability detection and automated repair suggestions; however, their adoption is constrained by inconsistent detection quality, high false-positive rates, outdated security knowledge, and significant privacy concerns when proprietary source code is processed by cloud-based services.

This thesis investigates a privacy-preserving approach to source code vulnerability detection and repair by integrating locally deployed LLMs with Retrieval-Augmented Generation (RAG) in a Visual Studio Code extension. The proposed system operates entirely on-device, ensuring zero code exfiltration while dynamically incorporating up-to-date vulnerability knowledge from curated Common Weakness Enumeration (CWE), Common Vulnerabilities and Exposures (CVE), and OWASP sources. Two operational modes are designed: a low-latency inline mode using LLM-only inference for real-time feedback, and an audit mode that combines LLM reasoning with retrieval-based context enrichment for deeper analysis.

The system is evaluated on a project-authored curated JavaScript/TypeScript dataset aligned to CWE/OWASP concepts. Quantitative evaluation compares LLM-only and LLM+RAG configurations across precision, recall, F1-score, false-positive rate, JSON parse success, and latency. Results show a strong model-dependent trade-off: the smallest model provides very low latency but low recall, mid-sized models increase recall with high alert noise, and the highest F1 score (42.70\%) is achieved by \texttt{qwen3:8b} with RAG at multi-second latency. The run also shows that parse robustness can dominate practical quality for some model families. Overall, the results demonstrate that privacy-preserving, locally deployed LLM workflows can provide useful vulnerability analysis assistance, while model selection, parse robustness, and output constraints remain critical for practical reliability.

\medskip
\noindent\textbf{Keywords:} source code security, vulnerability detection, secure code repair, large language models, retrieval-augmented generation, privacy-preserving systems, Visual Studio Code
